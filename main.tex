\documentclass[a4paper,12pt]{article}

%%% Работа с русским языком

\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в формулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\usepackage{indentfirst}            % красная строка в первом абзаце
\usepackage[unicode]{hyperref}
\usepackage{epigraph}
\frenchspacing                      % равные пробелы между словами и предложениями

%%% Дополнительная работа с математикой
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % пакеты AMS
\usepackage{bbm} % Blackboard bold для цифр
\usepackage{icomma}                                    % "Умная" запятая

\renewcommand{\phi}{\ensuremath{\varphi}}
\renewcommand{\kappa}{\ensuremath{\varkappa}}
\renewcommand{\le}{\ensuremath{\leqslant}}
\renewcommand{\leq}{\ensuremath{\leqslant}}
\renewcommand{\ge}{\ensuremath{\geqslant}}
\renewcommand{\geq}{\ensuremath{\geqslant}}
\renewcommand{\emptyset}{\ensuremath{\varnothing}}

\newcommand{\cl}{\text{cl }}
\newcommand{\setint}{\text{int }}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\theoremstyle{plain}
\newtheorem{theorem}{Теорема}[section]
\newtheorem{lemma}{Лемма}[section]
\newtheorem{proposition}{Утверждение}[section]
\newtheorem*{corollary}{Следствие}
\newtheorem*{exercise}{Упражнение}

\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newtheorem*{note}{Замечание}
\newtheorem*{reminder}{Напоминание}
\newtheorem*{example}{Пример}
\newtheorem*{tasks}{Вопросы и задачи}

\theoremstyle{remark}
\newtheorem*{solution}{Решение}

%%% Оформление страницы
\usepackage{extsizes}     % Возможность сделать 14-й шрифт
\usepackage{geometry}     % Простой способ задавать поля
\usepackage{setspace}     % Интерлиньяж
\usepackage{enumitem}     % Настройка окружений itemize и enumerate
\usepackage{epigraph}     % Эпиграф
\setlist{leftmargin=25pt} % Отступы в itemize и enumerate

\geometry{top=25mm}    % Поля сверху страницы
\geometry{bottom=30mm} % Поля снизу страницы
\geometry{left=20mm}   % Поля слева страницы
\geometry{right=20mm}  % Поля справа страницы

\begin{document}
\tableofcontents
\newpage

\section{Виды сходимости случайных векторов и связи между ними}
Пусть $\xi,\, \{\xi_n\}_{n = 1}^\infty$ -- случайные векторы размерности m.
\begin{definition}
  Сходимость \textbf{почти наверное}:
  \[
    \xi_n \overset{\text{п.н.}}{\to} \xi \Leftrightarrow P(\xi_n \to \xi) = 1
  \]
\end{definition}

\begin{definition}
  Сходимость \textbf{по вероятности}:
  \[
    \xi_n \overset{\text{P}}{\to} \xi \Leftrightarrow \forall \varepsilon > 0 :\: \lim_{n \to \infty}P(\|\xi_n - \xi\|_2 > \varepsilon) = 0 
  \]
\end{definition}

\begin{definition}
  Сходимость в $L_p$ \textbf{(в среднем)}:
  \[
    \xi_n \overset{L_p}{\to} \xi \Leftrightarrow \lim_{n \to \infty} \mathbb{E}\|\xi_n - \xi\|_p^p = 0
  \]
\end{definition}

\begin{definition}
  Сходимость по распределению:
  \[
    \xi_n \overset{\text{d}}{\to} \xi \Leftrightarrow \forall f \in \text{BC}(\mathbb{R}^m) :\: \mathbb{E}f(\xi_n) \overset{n \to \infty}{\to} \mathbb{E}f(\xi)
  \]
\end{definition}

\begin{proposition}
  Связь между сходимостями:
  \begin{enumerate}
    \item п.н. $\Rightarrow P$
    \item $L_p \Rightarrow P$
    \item $P \Rightarrow d$
  \end{enumerate}
\end{proposition}

\begin{proposition}
  $\xi_n \overset{d}{\to} const \Rightarrow \xi_n \overset{P}{\to} const$
\end{proposition}

\begin{proposition}
  Связь между сходимостью векторов и сходимостью их компонент:
  \begin{enumerate}
    \item \[
      \xi_n \overset{\text{п.н.}}{\to} \xi \Leftrightarrow \forall i :\: \xi_n^{(i)} \overset{\text{п.н.}}{\to} \xi^{(i)}
    \]
    \item \[
      \xi_n \overset{\text{P}}{\to} \xi \Leftrightarrow \forall i :\: \xi_n^{(i)} \overset{\text{P}}{\to} \xi^{(i)}
    \]
    \item \[
      \xi_n \overset{L_p}{\to} \xi \Leftrightarrow \forall i :\: \xi_n^{(i)} \overset{L_p}{\to} \xi^{(i)}
    \]
    \item \[
      \xi_n \overset{d}{\to} \xi \Rightarrow \forall i :\: \xi_n^{(i)} \overset{d}{\to} \xi^{(i)}
    \]
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item \[
      \cap_{i = 1}^m \{\xi_n^{(i)} \to \xi^{(i)}\} = \{\xi_n \to \xi\} \subset \{\xi_n^{(i)} \to \xi^{(i)}\}
    \]
    Тогда для $\Rightarrow$ используем включение и свойство меры:
    \[
      1 = P(\{\xi_n \to \xi\}) \leq P(\{\xi_n^{(i)} \to \xi^{(i)}\})
    \]
    А для $\Leftarrow$:
    \[
      1 = P(\cap_{i = 1}^m \{\xi_n^{(i)} \to \xi^{(i)}\}) = P(\{\xi_n \to \xi\})
    \]
    \item Для $\Rightarrow$:
    \[
      \{\vert \xi_n^{(i)} - \xi^{(i)}\vert > \varepsilon\} \subset \{\|\xi_n - \xi\|_2 > \varepsilon\}
    \]
    А для $\Leftarrow$:
    \[
      \{\|\xi_n - \xi\|_2 > \varepsilon\} \subset \bigcup_{i = 1}^m\left\{\vert \xi_n^{(i)} - \xi^{(i)}\vert > \frac{\varepsilon}{\sqrt{m}}\right\}
    \]
    \item Заметим, что
    \[
      \forall i :\: \lim_{n \to \infty}\mathbb{E}\vert \xi_n^{(i)} - \xi^{(i)}\vert^p = 0 \Leftrightarrow \lim_{n \to \infty}  \mathbb{E}\| \xi_n - \xi\|^p_p = \lim_{n \to \infty}  \mathbb{E}\sum_{i = 1}^n\vert \xi_n^{(i)} - \xi^{(i)}\vert^p = 0 
    \]
    \item Для $\Rightarrow$ в качестве $f$ возьмём функцию-проектор.
  \end{enumerate}
\end{proof}

\begin{theorem}
  О наследовании сходимостей.

  Пусть $\xi,\, \{\xi_n\}_{n = 1}^\infty$ -- случайные векторы в $\mathbb{R}^m$, причём $\exists B \in \mathcal{B}(\mathbb{R}^m) :\: P(\xi \in B) = 1$ и $h :\: \mathbb{R}^m \to \mathbb{R}^k$ непрерывна в каждой точке множества $B$. Тогда
  \begin{align}
    \xi_n \overset{\text{п.н.},\, P,\, d}{\to} \xi \Rightarrow h(\xi_n)\overset{\text{п.н.},\, P,\, d}{\to} h(\xi)
  \end{align}
\end{theorem}

\begin{proof}
  \begin{itemize}
    \item Случай п.н.:
    \[
      P(h(\xi_n) \to h(\xi)) \geq P(h(\xi_n) \to h(\xi),\, \xi \in B) \geq P(\xi_n \to \xi,\, \xi \in B) = 1
    \]
    \item Случай $P$:
    
    Пусть $h(\xi_n) \overset{P}{\not\to} h(\xi) \Rightarrow$:
    \[
      \exists \varepsilon_0,\,\delta_0,\, \{n_k\}_{k = 1}^\infty :\: P(\|h(\xi_{n_k}) - h(\xi)\| > \varepsilon_0) \geq \delta_0
    \]
    Но из неё мы можем выбрать $\{\xi_{n_{k_m}}\}_{m = 1}^\infty$, сходящуюся почти всюду (по прошлому семестру), но тогда мы получили противоречие с предыдущим пунктом доказательства.
    \item Докажем для непрерывных $h$:
    
    Тогда \[
      \forall f \in BC(\mathbb{R}^k) :\: f(h(x)) \in BC(\mathbb{R}^m)
    \]
    Значит мы можем взять $f \circ h$ в качестве функции из определения сходимости по распределению и получить требуемое.
  \end{itemize}
\end{proof}

\section{Закон больших чисел, усиленный закон больших чисел\dots}
\begin{theorem}
  ЗБЧ.

  Пусть $\{\xi_n\}_{n = 1}^\infty$ -- попарно некорелированные вектора и $\sup_{n,\, i} \mathbb{V}\xi_n^{(i)} \leq C$. Тогда
  \[
    \frac{s_n - \mathbb{E}s_n}{n} \overset{P}{\to} 0
  \]
  где $\{s_n\}_{n = 1}^\infty = \{\sum_{i = 1}^n \xi_i\}_{n = 1}^\infty$
\end{theorem}

\begin{theorem}
  УЗБЧ.

  Пусть $\{\xi_n\}_{n = 1}^\infty$ -- независимые одинаково распределённые, причём $\mathbb{E}\xi_1 < +\infty$. Тогда
  \[
    \frac{s_n}{n} \overset{\text{п.н.}}{\to} \mathbb{E}\xi_1
  \]
\end{theorem}

\begin{theorem}
  ЦПТ.

  Пусть $\{\xi_n\}_{n = 1}^\infty$ -- независимые одинаково распределённые, причём $\exists$ ковариационные матрица $\mathbb{V}\xi_1$. Тогда
  \[
    \sqrt{n}\left(\frac{s_n}{n} - \mathbb{E}\xi\right) \overset{d}{\to} \mathcal{N}(0,\, \mathbb{V}\xi_1)
  \]
\end{theorem}

\begin{lemma}
  Лемма Слуцкого.

  Пусть $\xi_n \overset{d}{\to} \xi$ и $\eta_n \overset{d}{\to} c \:(const)$. Тогда 
  \[
    \xi_n + \eta_n \overset{d}{\to} \xi + \eta ;\;\;\;\; \xi_n\cdot\eta_n \overset{d}{\to} \xi\cdot c
  \]
\end{lemma}

\begin{proof}
  По некому утверждению без доказательства, будет верно
  \[
    \begin{pmatrix}
      \xi_n\\
      \eta_n
    \end{pmatrix} \overset{d}{\to} \begin{pmatrix}
      \xi\\
      c
    \end{pmatrix}
  \]
  Тогда, применив теорему о наследовании сходимостей с функциями $+,\, \cdot$ всё получится.
\end{proof}

\begin{example}
  Применение леммы Слуцкого.

  Пусть $\xi_n \overset{d}{\to} \xi$ -- последовательность случайных величин и $H :\: \mathbb{R} \to \mathbb{R}$ -- дифференцируемая в точке $a$ и $b_n \to 0$, причём $b_n \neq 0$. Тогда
  \[
    \frac{H(a + \xi_nb_n) - H(a)}{b_n} \overset{d}{\to} H'(a)\xi
  \]
\end{example}

\begin{proof}
  Введём
  \[
    h(x) := \begin{cases}
      \frac{H(a + x) - H(a)}{x},\, x \neq 0\\
      H'(a),\, x = 0
    \end{cases}
  \]
  Тогда $h$ непрерывна в $0$.

  По лемме Слуцкого:
  \[
    b_n\xi_n \overset{d}{\to} 0
  \]
  По теореме о наследовании сходимости
  \[
    h(b_n\xi_n) \overset{d}{\to} h(0) = H'(a) \Rightarrow \frac{H(a + \xi_nb_n) - H(a)}{b_n} = h(b_n\xi_n)\xi_n \overset{d}{\to} H'(a)\xi
  \]
\end{proof}

\begin{theorem}
  Обобщение на многомерный случай.

  Пусть $\xi_n \overset{d}{\to} \xi$ в $\mathbb{R}^m$, и $H :\: \mathbb{R}^m \to \mathbb{R}^s$, у которой в точке $a \in \mathbb{R}^m \: \exists$ матрица частных производных $H'(x) = \left(\frac{\partial H_i}{\partial x_j}\right)_{i = 1,\,j = 1}^{s,\, m}$, а также числовая последовательность
  $b_n \to 0,\, b_n \neq 0$. Тогда
  \[
    \frac{H(a + \xi_nb_n) - H(a)}{b_n} \overset{d}{\to} H'(a)\xi
  \]
\end{theorem}

\section{Вероятно-статистическая модель\dots}
Пусть $(\Omega,\, \mathcal{F})$ и $(E,\, \mathcal{E})$ -- измеримые пространства.

\begin{definition}
  Если $\xi :\: \Omega \to E$ такова, что
  \[
    \forall B \in \mathcal{E} :\: \xi^{-1}(B) \in \mathcal{F}
  \]
  то $\xi$ называется \textbf{случайным элементом}.

  Если $(E,\, \mathcal{E}) = (\mathbb{R}^m,\, \mathcal{B}(\mathbb{R}^m))$, то $\xi$ называется \textbf{случайным вектором}.

  Более того, если $m = 1$, то $\xi$ называется \textbf{случайном величиной}.
\end{definition}

\begin{definition}
  \textbf{Распределением} случайного элемента $\xi$ называется мера $P_\xi$ на $\mathcal{E}$, такая что $P_\xi(B) = P(\xi \in B)$
\end{definition}

\begin{definition}
  \textbf{Выборочное} пространство $\mathcal{X}$ -- множество всевозможных исходов одного эксперимента (обычно $\mathbb{R}^m$).

  $\mathcal{B}_\mathcal{X}$ -- $\sigma$-алгебра на $\mathcal{X}$ будем считать Барелевской.
\end{definition}

\begin{proposition}
  Построим модель эксперимента, как случайной величины.

  Пусть 
  \[
    \forall x \in \mathcal{X} :\: X(x) = x
  \]
  получим отображение $X :\: \mathcal{X} \to \mathcal{X}$, которое является случайным элементом на вероятностном пространстве $(\mathcal{X},\, \mathcal{B}_\mathcal{X},\, P)$ и имеет распределение $P_X = P$
\end{proposition}

\begin{proof}
  Проверим, что данная случайная величина действительно имеет необходимое нам распределение
  \[
    P_X(B) = P(X \in B) = P(x :\: X(x) \in B) = P(x \in B) = P(B)
  \]
\end{proof}

\begin{proposition}
  Построим модель $n$ независимых повторений нашего эксперимента.

  Рассмотрим $\mathcal{X}^n = \mathcal{X} \times \cdots \times \mathcal{X}$ и $\mathcal{B}_\mathcal{X}^n = \mathcal{B}(\mathcal{X}^n) = \sigma(B_1\times\cdots\times B_n),\, B_i \in \mathcal{B}_{\mathcal{X}}$, а $P^n = P \otimes\cdots\otimes P$ -- мера на $(\mathcal{X}^n,\, \mathcal{B}_\mathcal{X}^n)$, такая что $P^n(B_1\times\cdots\times B_n) = P(B_1)*\cdots* P(B_n)$.

  Для этого рассмотрим тождественное отображение $X :\: \mathcal{X}^n \to \mathcal{X}^n$. Его $i$-я компонента $X_i$ (по сути $i$-й проектор) является случайным вектором с распределением $P$, причём $X_1,\,\cdots,\,X_n$ независимы в совокупности.
\end{proposition}

\begin{proof}
  Фиксируем $i$, рассмотрим вероятность
  \begin{align*}
    P^n(X_i \in B_i) = P^n((x_1,\,\cdots,\,x_n) \in \mathcal{X} :\: X_i(x_1,\,\cdots,\,x_n) \in B_i) =
    P^n((x_1,\,\cdots,\,x_n) \in \mathcal{X} :\: x_i \in B_i) =\\ 
    P^n(\mathcal{X}\times\cdots\times B_i\times\cdots\times \mathcal{X}) = 1*\cdots*P(B_i)*\cdots*1
  \end{align*}
  Теперь докажем независимость:
  \begin{align*}
    P^n(X_1 \in B_1,\,\cdots,\,X_n\in B_n) = P^n((x_1,\,\cdots,\,x_n) \in \mathcal{X} :\: X_1(x_1,\,\cdots,\,x_n) \in B_1,\,\cdots) = \\
    P^n(B_1\times\cdots\times B_n) = \prod_{i = 1}^nP(B_i) = \prod_{i = 1}^nP^n(X_i \in B_i)
  \end{align*}
\end{proof}

\begin{definition}
  Совокупность $X = (X_1,\,\cdots,\,X_n)$ независимых одинаково распределённых случайных величин (или векторов) с распределением $P$ называется \textbf{выборкой} размера $n$ из распределения $P$.

  Также выборку $X$ иногда будем называть \textbf{наблюдением}.
\end{definition}

\begin{note}
  Для бесконечных выборок определим $\mathcal{X}^\infty = \mathcal{X} \times \mathcal{X} \times\cdots$ и $\mathcal{B}_\mathcal{X}^\infty = \sigma(\{B_1\times \cdots\times B_n\times \mathcal{X}\times\mathcal{X}\times\mathcal{X}\times\cdots\}_{n = 1}^\infty)$, а меру $P^\infty(B_1\times\cdots\times B_n\times\mathcal{X}\times\cdots) = P(B_1)*\cdots*P(B_n)$, такая мера существует и единственна.

  Аналогично предыдущим пунктам определяем \textbf{бесконечную серию эскпериментов}.
\end{note}

\begin{definition}
  Тройка $(\mathcal{X},\, \mathcal{B}_\mathcal{X},\, \mathcal{P})$ называется \textbf{вероятностно-статистической моделью}.
\end{definition}

\begin{note}
  Пусть $X_1,\,\cdots,\,X_n$ -- случайные величины (или векторы), и $X_1(\omega) = x_1,\, \cdots,\, X_n(\omega) = x_n$ -- их значения, называются \textbf{реализацией выборки}.

  \textbf{Задачей статистики} является сделать вывод о неизвестном распределении по реализации выборки.
\end{note}

\begin{definition}
  Вероятно-статистическая модель $(\mathcal{X},\, \mathcal{B}_\mathcal{X},\, \mathcal{P})$ называется \textbf{параметрической}, если семейство $\mathcal{P}$ параметризованно, то есть
  \[
    \mathcal{P} = \{P_\theta,\, \theta \in \Theta\}
  \]
  обычно $\Theta \subset \mathbb{R}^m$.
\end{definition}

\section{Эмпирическое распределение и эмпирическая функция распределения}

\begin{definition}
  Для $\forall B \in \mathcal{B}(\mathbb{R}^m)$ положим 
  \[
    P_n^*(B) := \frac{\sum_{i = 1}^n \mathbb{I}\{X_i \in B\}}{n}
  \]
  распределение $P_n^*$ называется \textbf{эмпирическим распределением}, построенным по выборке $X_1,\,\cdots,\,X_n$.

  Это случайное распределение (зависит от $\omega$)
\end{definition}

\begin{definition}
  Функция $F_n^*(x) = \frac{\sum_{i = 1}^n \mathbb{I}\{X_i \leq x\}}{n}$ называется \textbf{эмпирической функцией распределения}.
\end{definition}

\begin{proposition}
  Пусть $X_1,\,\cdots,\,X_n$ -- выборка на вероятностном пространстве $(\Omega,\,\mathcal{F},\, P)$ из распределения $P_X$. Пусть $B \in \mathcal{B}(\mathbb{R}^m)$. Тогда
  \[
    P_n^*(B) \overset{\text{п.н.}}{\to} P_X(B),\, n \to +\infty
  \]
\end{proposition}

\begin{proof}
  Заметим, что $\mathbb{I}\{X_i \in B\}$ -- независимые, одинаково распределённые величины.

  Тогда мы можем применить УЗБЧ:
  \[
    P_n^*(B) = \frac{s_n}{n } \overset{\text{п.н.}}{\to}\mathbb{E}\mathbb{I}\{X_1 \in B\} = P_X(B)
  \]
\end{proof}

\begin{theorem}
  Гливенко-Кантелли.

  Пусть $X_1,\,\cdots,\,X_n$ -- независимые случайные величины с функцией распределения $F(x)$. Тогда 
  \[
    D_n = \sup_{x \in \mathbb{R}}\vert F_n^*(x) - F(x)\vert \overset{\text{п.н.}}{\to} 0
  \]
\end{theorem}

\begin{proof}
  Почему $D_n$ -- случайная величина? 

  $F$ непрерывна справа, и $\forall \omega :\: F_n^*$ также непрерывна справа $\Rightarrow$ 
  \[
    D_n(\omega) = \sup_{x \in \mathbb{R}}\vert F_n^*(x) - F(x)\vert = \sup_{x \in \mathbb{Q}}\vert F_n^*(x) - F(x)\vert
  \]
  Значит $D_n$ является случайной совокупностью случайных величин $\Rightarrow D_n$ -- случайная величина.

  Фиксируем $N \in \mathbb{N}$, тогда $\forall k \in \{1,\, \cdots,\, N - 1\}$ положим 
  \[
    X_{N,\, K} = \inf\left\{x \in \mathbb{R} \:\vert\: F(x) \geq \frac{K}{N}\right\}
  \]
  Заметим, что это число конечно, а также определим $X_{N,\, 0} = -\infty,\, X_{N,\, N} = +\infty$.

  Если $x \in [X_{N,\, K},\, X_{N,\, K + 1}) \Rightarrow$
  \begin{align*}
    F_n^*(x) - F(x) \leq F_n^*(X_{N,\, K + 1} - 0) - F(X_{N,\, K}) =\\
    F_n^*(X_{N,\, K + 1} - 0) - F(X_{N,\, K + 1} - 0) + F(X_{N,\, K + 1} - 0) - F(X_{N,\, K}) \leq\\
    F_n^*(X_{N,\, K + 1} - 0) - F(X_{N,\, K + 1} - 0) + \frac{1}{N}
  \end{align*}
  Последний переход получили благодаря тому, что $F(X_{N,\, K + 1} - 0)$ -- отсуп чуть влево, от нижней границы значения, где $F(x) \geq \frac{K + 1}{N}$, значит там $\leq \frac{K + 1}{N}$. Ну а $F(X_{N,\, K})$ по определению $\geq \frac{K}{N}$.

  Аналогично $F_n^*(x) - F(x) \geq F_n^*(X_{N,\, K}) - F(X_{N,\, K}) - \frac{1}{N}$. Тогда
  \[
    \vert F_N^*(x) - F(x)\vert \leq \max(\vert F_n^*(X_{N,\, K + 1} - 0) - F(X_{N,\, K + 1} - 0)\vert,\, \vert F_n^*(X_{N,\, K}) - F(X_{N,\, K})\vert) + \frac{1}{N}
  \]
  Но тогда супремум по всей прямой
  \[
    \sup_{x \in \mathbb{R}}\vert F_N^*(x) - F(x)\vert \leq \max_{0 \leq K \leq N - 1}\max(\vert F_n^*(X_{N,\, K + 1} - 0) - F(X_{N,\, K + 1} - 0)\vert,\, \vert F_n^*(X_{N,\, K}) - F(X_{N,\, K})\vert) + \frac{1}{N}
  \]
  Из предыдущего утверждения следует, что $F_n^*(y - 0) = P_n^*((-\infty,\, y)) \to P_X((-\infty,\, y)) = F(y - 0)$.

  Теперь для $\varepsilon$ фиксируем $\frac{1}{N} < \varepsilon \Rightarrow$
  \[
    \overline{\lim}_N \sup_{x \in \mathbb{R}}\vert F_N^*(x) - F(x)\vert \overset{\text{п.н.}}{<} \varepsilon
  \]
  В силу произвольности $\varepsilon$ получаем требуемое.
\end{proof}

\section{Статистики и оценки}
\begin{definition}
  Пусть $(\mathcal{X},\, \mathcal{B}_\mathcal{X},\, \mathcal{P})$ -- вероятно-статистическая модель, $X$ -- наблюдение, $(E,\,\mathcal{E})$ -- измеримое пространство, и $S :\: \mathcal{X} \to E$ -- измеримое отображение. Тогда $S(x)$ называется \textbf{статистикой}.
\end{definition}

\begin{definition}
  Пусть $X$ -- наблюдение в параметрической модели $(\mathcal{X},\, \mathcal{B}_\mathcal{X},\, \{P_\theta\}_{\theta \in \Theta})$ и $S(X)$ -- статистика со значениями в $\Theta$. Тогда $S(X)$ называется \textbf{оценкой} неизвестного параметра $\Theta$.
\end{definition}

\begin{example}
  Пусть $X = (X_1,\,\cdots,\,X_n)$ -- выборка из распределения в $\mathbb{R}^n$. 
  \begin{enumerate}
    \item Если $g(x)$ -- борелевская функция, то
    \[
      \overline{g(X)} = \frac{1}{n}\sum_{i = 1}^n g(X_i)
    \]
    называется выборочной характеристикой функции $g(x)$. Например $\overline{X} = \frac{\sum X_i}{n}$ -- выборочное среднее. $\overline{X^k} = \frac{1}{n}\sum_{i = 1}^n X_i^k$ -- выборочный момент $k$-го порядка.
    \item Функции от выборочных квантилей:
    \[
      S(X) = h(\overline{g_1(X)},\,\cdots,\,\overline{g_k(X)})
    \]
    где $h$ -- борелевская.

    Например, $s^2 = \overline{X^2} - (\overline{X})^2$ -- выборочная дисперсия. $M_k = \frac{1}{n}\sum(X_i - \overline{X})^k$ -- выборочный центральный момент $k$-го порядка.
    \item Порядковые статистики:
    \begin{align*}
      X_{(1)} = \min(X_1,\,\cdots,\,X_n)\\
      X_{(2)} - \text{второй элемент в отсортированной выборке}\\
      X_{(n)} = \max(X_1,\,\cdots,\,X_n)
    \end{align*}
    вектор $(X_{(1)},\,\cdots,\,X_{(n)})$ называется \textbf{вариационным рядом}.
  \end{enumerate}
\end{example}

Пусть $X = (X_1,\,\cdots,\,X_n)$ -- выборка из неизвестного распределения $P \in \{P_\theta,\, \theta \in \Theta\},\, \Theta \subset \mathbb{R}^k$.

\begin{definition}
  Оценка $\theta^*(X)$ называется \textbf{несмещённой} оценкой параметра $\theta$, если
  \[
    \forall \theta \in \Theta :\: \mathbb{E}_\theta\theta^*(X) = \theta
  \]
  где $\mathbb{E}_\theta$ -- матожидание в случае, когда элементы выборки имеют распределение $P_\theta$.
\end{definition}

\begin{definition}
  Оценка $\theta_n^*(X_1,\,\cdots,\,X_n)$ (а точнее последовательность оценок) называется \textbf{состоятельной}, если
  \[
    \forall \theta \in \Theta :\: \theta^*(X) \overset{P_\theta}{\to} \theta
  \]
  и называется \textbf{сильно состоятельной} если
  \[
    \forall \theta \in \Theta :\: \theta^*(X) \overset{P_\theta\text{- п. н.}}{\to} \theta
  \]
\end{definition}

\begin{definition}
  Оценка $\theta^*(X_1,\,\cdots,\,X_n)$ называется \textbf{асимптотически нормальной} оценкой $\theta$, если 
  \[
    \forall \theta \in \Theta :\: \sqrt{n}(\theta_n^* - \theta) \overset{d_\theta}{\to} \mathcal{N}(0,\, \sigma^2(\theta))
  \]
\end{definition}

\begin{proposition}
  Пусть $T(X)$ -- асимптотически нормальная оценка для $\tau(\theta)$. Тогда $T(X)$ -- состоятельная оценка для $\tau(\theta)$.
\end{proposition}

\begin{proof}
  Используя лемму Слуцкого, получаем
  \[
    \frac{1}{\sqrt{n}}\cdot\sqrt{n}(T_n - \tau(\theta)) \overset{d_\theta}{\to} 0 
  \]
  Но мы знаем, что из сходимости по распределению к константе следует сходимость по мере.
\end{proof}

\begin{proposition}
  Из сильной состоятельности и асимптотической нормальности оценки следует её состоятельность.
\end{proposition}

\begin{proof}
  Следствие из сильной состоятельности автоматически следует из связи сходимостей.

  Следствите из асимптотической нормальности было доказано в предыдущем утверждении.
\end{proof}

\section{О наследовании состоятельностей}
\begin{proposition}
  Наследование состоятельности и сильной состоятельности при взятии непрерывной функции.

  Пусть $\theta_n^*(X)$ -- сильно состоятельная (состоятельная) оценка $\theta$. Если $\tau:\:\mathbb{R}^k \to \mathbb{R}^s$ непрерывна на $\Theta \subset \mathbb{R}^k$, то $\tau(\theta_n^*)$ -- сильно состоятельная (состоятельная) оценка $\tau(\theta)$.
\end{proposition}

\begin{proof}
  Смотри доказательство теоремы о наследовании сходимости.
\end{proof}

\begin{lemma}
  О наследовании асимптотической нормальности.

  Пусть $\theta_n^*(X)$ -- асимптотически нормальная оценка $\theta \in \Theta$ с асимптотической дисперсией $\sigma^2(\theta)$ и числовая функция $T:\: \mathbb{R} \to \mathbb{R}$ дифференцируема в $\forall \theta \in \Theta$. Тогда $T(\theta^*_n)$ -- асимптотически нормальная оценка $T(\theta)$ с асимптотической дисперсией $\sigma^2(\theta)(T'(\theta))^2$
\end{lemma}

\begin{proof}
  Фиксируем $\theta,\, \xi_n := \sqrt{n}(\theta_n^*(X) - \theta) \overset{d_\theta}{\to} \xi \sim \mathcal{N}(0,\, \sigma^2(\theta)),\, b_n := \frac{1}{\sqrt{n}} \to 0$.

  Вспомним дельта метод, взяв 
  \[
    a = \theta,\, h = T \Rightarrow \frac{T(\theta + \xi_nb_n) - T(\theta)}{b_n} \overset{d_\theta}{\to} T'(\theta)\xi \Rightarrow \sqrt{n}(T(\theta_n^*) - T(\theta)) \overset{d_\theta}{\to} T'(\theta)\mathcal{N}(0,\, \sigma^2(\theta))
  \] 
\end{proof}

\section{Метод подстановки и метод моментов}
\begin{definition}
  Пусть в параметрическом семействе $\{P_\theta,\, \theta \in \Theta\}$ для некоторой функции $G$ выполнено:
  \[
    \forall \theta \in \Theta :\: \theta = G(P_\theta)
  \]
  Тогда оценкой по \textbf{методу подстановки} называется $\theta^*(X_1,\,\cdots,\,X_n) = G(P_n^*)$
\end{definition}

Пусть $X_1,\,\cdots,\,X_n$ -- выборка из $P \in \{P_\theta,\, \theta \in \Theta\},\, \Theta \subset \mathbb{R}^k$. Рассмотрим барелевские функции $g_1(x),\,\cdots,\,g_k(x)$ со значениями в $\mathbb{R}$.

Пусть $m_1(\theta) = \mathbb{E}_\theta g_i(X_1)$ конечно при $1 \leq i \leq k$. 

\begin{definition}
  Если $\exists!$ решение системы
  \[
    \begin{cases}
      m_1(\theta) = \overline{g_1(X)}\\
      \dots\\
      m_k(\theta) = \overline{g_k(X)}
    \end{cases}
  \]
  Тогда оценкой по \textbf{методу моментов} называется $\theta^* = m^{-1}(\overline{g})$, где
  \[
    m(\theta) := \begin{pmatrix}
      m_1(\theta)\\
      \vdots\\
      m_k(\theta)
    \end{pmatrix};\;\;\;
    \overline{g} = \begin{pmatrix}
      \frac{\sum_{i = 1}^n g_1(X_i)}{n}\\
      \vdots\\
      \frac{\sum_{i = 1}^n g_k(X_i)}{n}
    \end{pmatrix}
  \]
  Стандартные \textbf{пробные функции}: $g_i(X) = X^i$ ($i$-й момент).
\end{definition}

\begin{note}
  О связи методов.

  Заметим, что
  \[
    \theta = m^{-1}\begin{pmatrix}
      \int_\mathcal{X} g_1(x)dP_\theta(x)\\
      \vdots\\
      \int_\mathcal{X} g_k(x)dP_\theta(x)
    \end{pmatrix} = G(P_\theta)
  \]
  Тогда по методу подстановки получим
  \[
    \theta_n^* = m^{-1}\begin{pmatrix}
      \int_\mathcal{X} g_1(x)dP^*_n(x)\\
      \vdots\\
      \int_\mathcal{X} g_k(x)dP_n^*(x)
    \end{pmatrix} = G(P_n^*)
  \]
  Таким образом, метод моментов -- это частный случай метода подстановки.
\end{note}

\begin{theorem}
  Сильная состоятельной оценки методом моментов.

  Если $m$ биективна и функцию $m^{-1}$ можно доопределить до функции, заданной на всём $\mathbb{R}^k$ и непрерывной в каждой точке множества $m(\Theta)$ тогда оценка по методу моментов является сильно состоятельной оценкой параметра $\theta$.
\end{theorem}

\begin{proof}
  Фиксируем $\theta$, по УЗБЧ знаем, что
  \[
    \overline{g} \overset{P_\theta \text{ п.н.}}{\to} m(\theta)
  \]
  Используя теорему о наследовании сходимости, навесим $m^{-1}$:
  \[
    \theta_n^* = m^{-1}(\overline{g}) \overset{P_\theta \text{ п.н.}}{\to} m^{-1}(m(\theta)) = \theta
  \] 
\end{proof}

\begin{theorem}
  Асимптотическая нормальность ОММ.

  Если в условиях предыдущей теоремы $m^{-1}$ дифференцируема на $m(\Theta)$ и $\forall i \leq k :\: \mathbb{E}_\theta g_i^2(X_1) < +\infty$. Тогда ОММ $\theta_n^*$ является асимптотически нормальной оценкой параметра $\theta$.
\end{theorem}

\begin{proof}
  По ЦПТ:
  \[
    \sqrt{n}(\overline{g} - m(\theta)) \overset{d_\theta}{\to} \mathcal{N}(0,\, \Sigma)
  \]
  Применяем многомерный дельта-метод и получаем требуемое.
\end{proof}

\section{Квантили и выборочные квантили}
\begin{definition}
  Пусть $P$ -- распределение вероятности на $\mathbb{R}$. Пусть $p \in (0,\, 1)$. \textbf{$p$-квантилью} распределения $P$ называют 
  \[
    z_p = \inf\{x \in \mathbb{R} \:\vert\: F(x) \geq p\}
  \]
\end{definition}

\begin{definition}
  Пусть $X_1,\,\cdots,\,X_n$ -- выборка, статистика
  \[
    z_{n,\,p} = \begin{cases}
      X_{(\lceil np\rceil)},\, np \not\in \mathbb{Z}\\
      X_{(np)},\, np \in \mathbb{Z}
    \end{cases}
  \]
  называется \textbf{выборочной $p$-квантилью}.
\end{definition}

\begin{theorem}
  О выборочной квантили.

  Пусть $X_1,\,\cdots,\,X_n$ -- выборка из распределения $P$ с плотностью $f(x)$. Пусть $z_p$ -- это $p$-квантиль распределения $P$, причём $f(x)$ непрерывно дифференцируема в окрестности $z_p$, причём $f(z_p) > 0$. Тогда
  \[
    \sqrt{n}(z_{n,\,p} - z_p) \overset{d}{\to} \mathcal{N}\left(0,\, \frac{p(1 - p)}{f^2(z_p)}\right)
  \]
\end{theorem}

\begin{proof}
  Пусть $k := \lceil np\rceil$.
  
  Из соображений комбинаторики, заметим, что
  \[
    P(X_{(k)} \leq x) = \sum_{m = k}^n C_n^m F^m(x)(1 - F(x))^{n - m}
  \]
  Засчёт свойств биномиальных коэффициентов, после дифференцирования выражения выше, получим
  \[
    p_{X_{(k)}}(x) = nC_{n-1}^{k-1}F^{k - 1}(x)(1 - F(x))^{n - k}f(x)
  \]
  Введём
  \[
    \eta_n = (z_{n,\,p} - z_p)\sqrt{\frac{nf^2(z_p)}{p(1 - p)}}
  \]
  Плотность такого линейного преобразования легко считается
  \[
    p_{\eta_n}(x) = \sqrt{\frac{p(1 - p)}{nf^2(z_p)}}p_{X_{(k)}}(t_n(x))
  \]
  где $t_n(x) = z_p + \frac{x}{f(z_p)}\sqrt{\frac{p(1 - p)}{n}}$

  Откуда это взялось? Вспомним, как меняется плотность при линейном преобразовании:
  \[
    p_{a\xi + b}(x) = F_{a\xi + b}'(x) = P'(a\xi + b \leq x) = P'(\xi \leq \frac{x - b}{a}) = F'_\xi(\frac{x - b}{a}) = \frac{1}{a}p_\xi(\frac{x - b}{a})
  \]
  Раскроем $p_{X_{(k)}}$ по формуле, которую получили в начале доказательства и разложим полученную плотность $\eta_n$ в следующее произведение:
  \[
    p_{\eta_n}(x) = A_1(n)A_2(n)A_3(n)
  \]
  где 
  \begin{align*}
  A_1(n) = \sqrt{npq}C_{n - 1}^{k - 1}p^{k - 1}q^{n - k}\\
  A_2(n) = \frac{f(t_n(x))}{f(z_p)}\\
  A_3(n) = \left(\frac{F(t_n(x))}{p}\right)^{k - 1}\left(\frac{1 - F(t_n(x))}{q}\right)^{n - k}
  \end{align*}
  Осталось заметить, что
  \[
    A_1(n) \to \frac{1}{\sqrt{2\pi}};\;\;\;\; A_2(n) \to 1;\;\;\;\; 
  \]
  Для $A_3(n)$ немного сложнее, разложим $F(t_n(x))$ в ряд Тейлора в окрестности $z_p$. (так как $t_n(x) \to z_p$):
  \[
    F(t_n(x)) = F(z_p) + (t_n - z_p)F'(z_p) + \frac{1}{2}(t_n - z_p)^2F''(z_p) + o(t_n - z_p)^2
  \]
  Давайте упростим это выражение, раскрыв $t_n$ и применив свойство квантиля $F(z_p) = p$:
  \[
    F(t_n(x)) = p + x\sqrt{\frac{pq}{n}} + \frac{1}{2}\frac{x^2pq}{n}\cdot\frac{f'(z_p)}{f^2(z_p)} + o(\frac{1}{n}),\, n \to +\infty
  \]
  Теперь должны расписать приближение $\ln\left(\frac{F(t_n(x))}{p}\right)$, используя формулу $\ln(1 + x) = x - \frac{x^2}{2} + o(x^3)$, причём в квадрате нам нужен будет только $x\sqrt{\frac{pq}{n}}$:
  \[
    \ln\left(\frac{F(t_n(x))}{p}\right) = x\sqrt{\frac{q}{pn}} + \frac{1}{2}\frac{x^2q}{n}\frac{f'(z_p)}{f^2(z_p)} + o\left(\frac{1}{n}\right) - \frac{x^2}{2}\frac{q}{np}
  \]
  Аналогично разложив для $\ln\left(\frac{1 - F(t_n(x))}{q}\right)$, получим
  \[
    \ln A_3(n) \to -\frac{x^2}{2}
  \]
  Таким образом, $p_{\eta_n(x)} \to \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ и эта сходимость равномерна на $\forall [-N,\, N]$.

  Используя теорему из теории вероятностей,
  \[
    \eta_n \overset{d}{\to} \mathcal{N}(0,\, 1)
  \]
\end{proof}

\begin{definition}
  \textbf{Медианой} распределения $P$ называется $\frac{1}{2}$ квантиль.

  \textbf{Выборочной медианой} называется
  \[
    \hat{\mu} = \begin{cases}
      X_{(k)},\, n = 2k + 1\\
      \frac{X_{(k)} + X_{(k + 1)}}{2},\, n = 2k
    \end{cases}
  \]
\end{definition}

\begin{theorem}
  О выборочной медиане.

  В условиях теоремы о выборочной квантили:
  \[
    \sqrt{n}(\hat{\mu} - z_{\frac{1}{2}}) \overset{d}{\to} \mathcal{N}\left(0,\, \frac{1}{4f^2(z_{\frac{1}{2}})}\right)
  \]
\end{theorem}

\section{Сравнение оценок, функция потерь и функция риска}
\begin{definition}
  Борелевская неотрицательная функция $g(x,\,y)$ называется \textbf{функцией потерь}.

  Если $\theta^*(X)$ -- оценка, то $g(\theta^*(X),\, \theta)$ называется \textbf{величиной потерь}.
\end{definition}

\begin{definition}
  Если задана функция потерь $g$, то \textbf{функцией риска} оценки $\theta^*$ называется $R(\theta^*,\, \theta) = \mathbb{E}_\theta g(\theta^*,\, \theta)$
\end{definition}

\begin{definition}
  Оценка $\theta^*(X)$ лучше оценки $\hat{\theta}(X)$ в \textbf{равномерном подходе}, если 
  \[
    \forall \theta \in \Theta :\: R(\theta^*(X),\, \theta) \leq R(\hat{\theta}(X),\, \theta)
  \]
  и для некоторого $\theta$ неравенство строгое.
\end{definition}

\begin{definition}
  Оценка $\theta^*(X)$ называется наилучшей в \textbf{минимаксном подходе}, если 
  \[
    \sup_{\theta \in \Theta}R(\theta^*(X),\, \theta) = \inf_{\hat{\theta}}\sup_{\theta \in \Theta}R(\hat{\theta}(X),\, \theta)
  \]
  то есть у $\theta^*(X)$ наименьший максимум функции риска.
\end{definition}

\begin{definition}
  Предположим, что на $\Theta$ задано некоторое \textbf{априорное} распределение вероятности $Q$ и $\theta$ выбирается случайно в соответствии с распределением $Q$.

  Если $\hat{\theta}(X)$ -- оценка $\theta$ и $R(\hat{\theta},\, \theta)$ -- её функция риска, тогда 
  \[
    R(\hat{\theta}(X)) = \mathbb{E}_\theta R(\hat{\theta}(X),\, \theta) = \int_\Theta R(\hat{\theta}(X),\, t)Q(dt)
  \]

  Оценка $\theta^*(X)$ называется наилучшей в \textbf{байесовском} подходе, если
  \[
    R(\theta^*(X)) = \min_{\hat{\theta}}R(\hat{\theta}(X))
  \]
\end{definition}

\begin{definition}
  Пусть $\hat{\theta}_1,\, \hat{\theta}_2$ -- две асимптотически нормальных оценки параметра $\theta$ с дисперсиями $\sigma_1^2(\theta),\, \sigma_2^2(\theta)$. 

  Оценка $\hat{\theta}_1$ лучше $\hat{\theta}_2$ в \textbf{асимптотическом подходе}, если
  \[
    \forall \theta \in \Theta :\: \sigma_1^2(\theta) \leq \sigma_2^2(\theta)
  \]
\end{definition}

\section{Понятие плотности в дискретном случае}
\begin{definition}
  \textbf{Считающей мерой} $\mu$ на $\mathbb{Z}$ называется функция $\mu :\: \mathcal{B}(\mathbb{R}) \to \mathbb{Z}_+ \cup \{+\infty\}$, определённая по правилу
  \[
    \mu(B) = \sum_{k \in \mathbb{Z}} \mathbb{I}\{k \in B\}
  \]
\end{definition}

\begin{definition}
  Интегралом по считающей мере от функции $f(x)$ называется 
  \[
    \int_\mathbb{R}f(x)\mu(dx) = \sum_{k \in \mathbb{Z}}f(k)
  \]
  если ряд в правой части сходится абсолютно.
\end{definition}

\begin{definition}
  Пусть $\xi$ -- дискретная случайная величина, принимающая значения в $\mathbb{Z}$. Её плотностью относительно считающей меры $\mu$ называется функция
  \[
    p(x) = P(\xi = x),\, x \in \mathbb{Z}
  \]
\end{definition}

\begin{note}
  Всюду далее, когда говорим о плотности, считаем, что либо это обычная плотность в абсолютно непрерывном случае, либо это плотность в дискретном случае по считающей мере на $\mathbb{Z}^n$.
\end{note}

\begin{definition}
  Пусть $X$ -- наблюдение из неизвестного распределения $P \in \{P_\theta,\, \theta \in \Theta\}$, причём $\forall \theta \in \Theta :\: p_\theta(x)$ имеет плотность $p_\theta(x)$ по одной и той же мере $\mu$.

  В этом случае семейство $\{P_\theta,\, \theta \in \Theta\}$ называется \textbf{доминируемым} относительно $\mu$.
\end{definition}

\begin{definition}
  Пусть $X$ -- наблюдение с неизвестным распределением $P \in \{P_\theta,\, \theta \in \Theta\}$, где семейство доминируемо относительно $\mu$ (значит, что либо все дискретные, либо все абсолютно непрерывные).

  \textbf{Функцией правдоподобия} называют
  \[
    f_\theta(X) := p_\theta(X)
  \]
  где $p_\theta(X)$ -- плотность $p_\theta$ по мере $\mu$.
\end{definition}

\begin{example}
  Пусть $X = (X_1,\,\cdots,\,X_n)$ -- выборка с плотностью $p_\theta(x)$, то $f_\theta(X) = \prod_{i = 1}^n p_\theta(X_i)$
\end{example}

\begin{definition}
  Определим
  \[
    L_\theta(X) = \ln f_\theta(X)
  \]
  называется \textbf{логарифмической функцией правдоподобия}.
\end{definition}

\begin{definition}
  Случайная величина $u_\theta(x) = \frac{\partial}{\partial \theta}L_{p_\theta}(x)$ называется \textbf{вкладом} наблюдения $X$, и функция $I_X(\theta) = \mathbb{E}_\theta u_\theta^2(X)$ называется \textbf{количеством информации} о параметре $\theta$ содержащемся в $X$ (информация \textbf{по Фишеру}).
\end{definition}

\begin{note}
  Будем считать, что выполнено условие \textbf{регулярности}:
  \begin{enumerate}
    \item $\Theta \subset \mathbb{R}$ -- открытый интервал
    \item Множество $A = \{x \in \mathcal{X} \:\vert\: p_\theta(x) > 0\}$ не зависит от $\theta$.
    \item Для $\forall$ статистики $S(X)$ с условием $\mathbb{E}_\theta S^2(X) < +\infty$ выполнено $\forall \theta$ выполнено 
    \[
      \frac{\partial}{\partial\theta}\int_AS(x)p_\theta(x)\mu(dx) = \int_AS(x)\frac{\partial}{\partial \theta}p_\theta(x)\mu(dx)
    \]
    Левая часть это $\frac{\partial}{\partial\theta}\mathbb{E}_\theta S(X)$, а правая часть
    \[
      \int_AS(x)\frac{\partial}{\partial \theta}p_\theta(x)\frac{1}{p_\theta(x)}p_\theta(x)\mu(dx) = \mathbb{E}_\theta S(X)\frac{\partial}{\partial\theta}\ln p_\theta(X) = \mathbb{E}_\theta S(X)u_\theta(X)
    \]
    \item $\forall \theta \in \Theta:\:0 < I_X(\theta) < +\infty$
  \end{enumerate}
\end{note}

\begin{theorem}
  Неравенство Рао-Крамера.

  Пусть выполнено условие регулярности и $\hat{\theta}(X)$ -- несмещённая оценка $\tau(\theta)$ с условием 
  \[
    \forall \theta \in \Theta:\:\mathbb{E}_\theta(\hat{\theta}(X))^2 < +\infty
  \]
  Тогда
  \[
    \mathbb{V}_\theta\hat{\theta}(X) \geq \frac{(\tau'(\theta))^2}{I_X(\theta)}
  \]
\end{theorem}

\begin{proof}
  В силу условия 3, при $S(X) = 1$ имеем
  \[
    \frac{\partial}{\partial\theta}\mathbb{E}_\theta S(X) = \frac{\partial}{\partial\theta}1 = \mathbb{E}_\theta u_\theta(X) = 0
  \]
  Также в силу условия 3, при $S(X) = \hat{\theta}(X)$ имеем (в силу несмещённости нашей оценки)
  \[
    \tau'(\theta) = \mathbb{E}_\theta\hat{\theta}(X)u_\theta(X)
  \]
  Умножим первое равенство на $-\tau(\theta)$ и сложим со вторым:
  \[
    \tau'(\theta) = \mathbb{E}(\hat{\theta} - \tau(\theta))u_\theta(X)
  \]
  Возведём обе части в квадрат и применим КБШ:
  \[
    (\tau'(\theta))^2 \leq \left(\mathbb{E}_\theta(\hat{\theta} - \tau(\theta))^2\right)\cdot\left(\mathbb{E}_\theta u_\theta^2(X)\right) = \mathbb{V}_\theta\hat{\theta}\cdot I_X(\theta)
  \]
\end{proof}

\begin{definition}
  Если в неравенстве Рао-Крамера для оценки $\hat{\theta}(X)$ достигается равенство, то $\hat{\theta}(X)$ называется \textbf{эффективной}.
\end{definition}

\begin{theorem}
  Критерий эффективности.

  В условиях регулярности $\hat{\theta}(X)$ эффективная для $\tau(\theta) \Leftrightarrow \hat{\theta}(X)$ -- линейная функция от $u_\theta(X)$ вида $\hat{\theta} - \tau(\theta) = c(\theta)u_\theta(X)$.

  Причём последнее равенство может быть выполнено $\Leftrightarrow c(\theta) = \frac{\tau'(\theta)}{I_X(\theta)}$
\end{theorem}

\begin{proof}
  Пусть $\hat{\theta}$ -- эффективная для $\tau(\theta) \Rightarrow \tau'(\theta) = \mathbb{E}(\hat{\theta} - \tau(\theta))u_\theta(X)$. А мы знаем, что равенство в КБШ достигается $\Leftrightarrow (\hat{\theta} - \tau(\theta))$ и $u_\theta(X)$ линейно зависимы:
  \[
    \alpha(\theta) + \beta(\theta)(\hat{\theta} - \tau(\theta)) + \gamma(\theta)u_\theta(X) = 0
  \]
  Матожидания рассматриваемых величин равны нулю $\Rightarrow \alpha(\theta) \equiv 0$.

  Можем поделить обе части на $\gamma(\theta) \neq 0$, это верно ведь иначе
  \[
    \mathbb{V}_\theta\beta(\theta)u_\theta(X) = 0 \Rightarrow \beta = 0 \Rightarrow\bot
  \]
  То есть $\hat{\theta} - \tau(\theta) = r(\theta)u_\theta(X)$.

  Обратно, пусть $\hat{\theta} - \tau(\theta) = c(\theta)u_\theta(X) \Rightarrow \hat{\theta} = \tau(\theta) + c(\theta)u_\theta(X) \Rightarrow \hat{\theta}$ -- несмещённая оценка $\tau(\theta)$. Умножим обе части на $u_\theta(X)$ и берём матож:
  \[
    \tau'(\theta) = \mathbb{E}_\theta(\hat{\theta} - \tau(\theta))u_\theta(X) = \mathbb{E}_\theta c(\theta)u^2_\theta(X) = c(\theta)I_X(\theta)
  \]
\end{proof}

\begin{note}
  Эффективная оценка $\tau(\theta)$ -- наилучшая оценка $\tau(\theta)$ в классе несмещённых $L_2$ оценок в равномерном подходе с квадратичной функцией потерь.
\end{note}

\section{Экспоненциальные семейства распределений}
\begin{definition}
  Пусть $\theta = (\theta_1,\,\cdots,\,\theta_k)$

  \textbf{Экспоненциальным семейством} распределений называют все распределения, обобщённая плотность которых имеет вид
  \[
    h(x)\exp\left(\sum_{i = 1}^ka_i(\theta)T_i(x) + V(\theta)\right)
  \]
  и где $a_0(\theta) \equiv 1,\, a_1(\theta),\,\cdots,\,a_k(\theta)$ линейно независимы на $\Theta$.
\end{definition}

\begin{note}
  Проверим, существует ли эффективная оценка, если семейство экспоненциальное:
  \[
    f_\theta(x) = \prod_{i = 1}^np_\theta(x_i);\;\;\;\; p_\theta(x_i) = h(x_i)e^{a(\theta)T(x_i) + V(\theta)}
  \]
  Тогда распишем вклад
  \[
    u_\theta(X) = \frac{\partial}{\partial\theta}\ln f_\theta(x) = \frac{\partial}{\partial\theta}(a(\theta)\sum_{i = 1}^nT(x_i) + nV(\theta)) = a'(\theta)\sum_{i = 1}^nT(x_i) + nV'(\theta)
  \]
  Работаем в предположении $T \neq const$, так как иначе
  \[
    p_\theta(x) = h(x)e^{b(\theta)} \Rightarrow \int_\mathcal{X}p_\theta(x)d\mu = 1 \Rightarrow b(\theta) = const \Rightarrow p_\theta(x) \text{ не зависит от }\theta
  \]
  Пусть также $a'(\theta) \neq 0$, тогда
  \[
    \frac{1}{na'(\theta)}u_\theta(x) = \frac{\sum_{i = 1}^n T(x_i)}{n} - \frac{-V'(\theta)}{a'(\theta)}
  \]
  По критерию эффективности получаем, что $T^*(X) = \frac{\sum_{i = 1}^n T(x_i)}{n}$ является эффективной оценкой для $\tau(\theta) = \frac{-V'(\theta)}{a'(\theta)}$

  Обратно, пусть $\exists$ эффективная оценка $T$ для $\tau(\theta)$, пусть $\forall \theta:\: \tau'(\theta) \neq 0$. Значит достигается равенство в Рау-Крамера:
  \[
    \exists \tau'(\theta) < +\infty :\: \mathbb{V}_\theta\hat{\theta} = \frac{(\tau'(\theta))^2}{I_X(\theta)} < +\infty \Rightarrow\hat{\theta} \in L_2
  \]
  Значит
  \[
    \forall \theta :\: T(X) - \tau(\theta) = c(\theta)u_\theta(x) = \frac{\tau'(\theta)}{I_X(\theta)}u_\theta(X) 
  \]
  Выразив вклад, получим
  \[
    \frac{\partial}{\partial\theta}\ln f_\theta(X) = \frac{T(X) - \tau(\theta)}{c(\theta)}
  \]
  Проинтегрируем, предполагая корректность:
  \[
    \ln f_\theta(X) = \int\frac{T(X) - \tau(\theta)}{c(\theta)}d\theta + g(X)
  \]
  Возведём экспоненту в обе части равенства и получим, что правдоподобие имеет нужный нам вид. Но как перейти от произведения плотностей с плотности определённого $X_i$? Зафиксируем остальные $X_j,\, j \neq i$ из носителя $A$ и заметим, что вид остался экспоненциальным.
\end{note}

\section{Достаточные статистики}
\begin{definition}
  Статистика $T(X)$ называется \textbf{достаточной} для параметра $\theta$, если 
  \[
    P_\theta(X \in B \:\vert\: T(X) = t)
  \]
  не зависит от $\theta$.
\end{definition}

\begin{theorem}
  Критерий факторизации Неймана-Фишера.

  Пусть $\{P_\theta,\, \theta \in \Theta\}$ -- доминирующее семейство. Статистика $T$ является достаточной для параметра $\theta \Leftrightarrow$ функция правдоподобия $f_\theta(X)$ представима в виде
  \[
    f_\theta(X) = \psi(T(X),\, \theta)h(X)
  \]
  где функции $\psi,\, h$ неотрицательны, $\psi(t,\, \theta)$ измерима по $t$ и $h$ измерима по $X$.
\end{theorem}

\begin{proof}
  Для дискретного случая.

  То есть $f_\theta(x) = P_\theta(X = x)$. Пусть $f_\theta(X) = \psi(S(X),\, \theta)h(X) \Rightarrow$
  \[
    P_\theta(X = x \:\vert\: T(X) = t) = \frac{P_\theta(X = x,\, T(X) = t)}{P_\theta(T(X) = t)} = \begin{cases}
      0,\, T(X) \neq t\\
      \frac{P_\theta(X = x)}{\sum_{y :\: T(y) = t}P_\theta(X = y)} = \frac{\psi(T(X),\, \theta)h(X)}{\sum_{y :\: T(y) = t} \psi(T(y),\, \theta)h(y)}
    \end{cases} 
  \]
  После сокращения имеем
  \[
    P_\theta(X = x \:\vert\: T(X) = t) = \begin{cases}
      0,\, T(X) \neq t\\
      \frac{h(X)}{\sum_{y :\: T(y) = t} h(y)},\, T(X) = t
    \end{cases}
  \]
  То есть получили что-то, независящее от $\theta$, что подходит под определение достаточной статистики.

  Обратно, пусть статистика $T$ достаточная:
  \begin{align*}
    f_\theta(x) = P_\theta(X = x) = P_\theta(X = x,\, T(X) = T(x)) =\\ 
    P_\theta(T(X) = T(x))\cdot P_\theta(X = x \:\vert\: T(X) = T(x)) = \psi(T(x),\, \theta)h(x)
  \end{align*}
\end{proof}

\begin{lemma}\label{UMO_uneq}
  Пусть $\eta \in L_1$, тогда $\mathbb{E}(\mathbb{E}(\eta \:\vert\: \xi) - \mathbb{E}\eta)^2 \leq \mathbb{V}\eta$.

  Более того, если $\eta \in L_2$, то равенство в неравенстве выше достигается $\Leftrightarrow \eta = \mathbb{E}(\eta\:\vert\: \xi) \Leftrightarrow \eta$ является $\xi$-измеримой.
\end{lemma}

\begin{proof}\label{KBR}
  Докажем лишь для $L_2$.

  Пусть $\phi = \mathbb{E}(\eta \:\vert\: \xi)$. Тогда по неравенству Йенсена
  \[
    \phi^2 = (\mathbb{E}(\eta\:\vert\:\xi))^2 \leq \mathbb{E}(\eta^2 \:\vert\: \xi)
  \]
  Навесив матожидание, получим $\mathbb{E}\phi^2 \leq \mathbb{E}\eta^2 < +\infty$. Далее,
  \[
    \mathbb{V}\eta = \mathbb{E}(\eta - \mathbb{E}\eta)^2 = \mathbb{E}(\eta - \phi + \phi - \mathbb{E}\eta)^2 = \mathbb{E}(\eta - \phi)^2 + \mathbb{E}(\phi - \mathbb{E}\eta)^2 + 2\mathbb{E}(\eta - \phi)(\phi - \mathbb{E}\eta)
  \]
  Распишем последнее слагаемое:
  \[
    \mathbb{E}(\mathbb{E}((\eta - \phi)(\phi - \mathbb{E}\eta) \:\vert\: \xi)) = \mathbb{E}(\phi - \mathbb{E}\eta)\mathbb{E}((\eta - \phi) \:\vert\: \xi) = 0
  \]
  Заметим, что мы всё доказали: оценим первое слагаемое нулём снизу и всё получится.
\end{proof}

\begin{theorem}
  Колмогорова-Блэкуэлла-Рао.

  Пусть $T(X)$ -- достаточная статистика для $\theta$ и пусть $d(X)$ -- несмещённая для $\tau(\theta)$, положим $\phi(T) = \mathbb{E}_\theta(d(X) \:\vert\: T)$. Тогда $\phi(T)$ зависит от выборки только через $T(X)$ (и не зависит от $\theta$), причём
  \[
    \mathbb{E}_\theta\phi(T) = \tau(\theta);\;\;\;\; \mathbb{V}_\theta\phi(T) \leq \mathbb{V}_\theta d(X)
  \]
\end{theorem}

\begin{proof}
  Рассмотрим $\phi(T) := \mathbb{E}_\theta(d(X) \:\vert\: T)$. Распределение $X$ (при фиксированном значении $T$) не зависит от $\theta \Rightarrow$ распределение $d(X)$ тоже не зависит $\Rightarrow \mathbb{E}_\theta(d(X) \:\vert\: T)$ является измеримой функцией только от $T$ (и, как функция, не зависит от $\theta$) $\Rightarrow \phi(T)$ действительно статистика.

  Очевидно, что $d(X)$ -- несмещённая $\Rightarrow \phi$ тоже (св-во УМО).
  \begin{align*}
    \mathbb{V}_\theta\phi(T) = \mathbb{E}_\theta(\phi - \mathbb{E}_\theta\phi)^2 = \mathbb{E}_\theta(\mathbb{E}_\theta(d\:\vert\:T) - \mathbb{E}_\theta d)^2 \overset{\text{по лемме}}{\leq} \mathbb{V}_\theta d(X)
  \end{align*}
  Если $d \in L_2 \Rightarrow$ неравенство переходит в равенство $\Leftrightarrow d = \phi\Leftrightarrow d(X)$ -- борелевская функция от $T$.
\end{proof}

\section{Полные статистики, оптимальные оценки}
\begin{definition}
  Наилучшая оценка $T(\theta)$ в классе несмещённых оценок в равномерном подходе с квадратичной функцией потерь называется \textbf{оптимальной} оценкой.
\end{definition}

\begin{definition}
  Статистика $S(X)$ называется \textbf{полной} для параметра $\theta$, если из условия 
  \[
    \forall \theta \in \Theta :\: \mathbb{E}_\theta f(S(X)) = 0
  \]
  следует, что
  \[
    \forall \theta \in \Theta :\: f(S(X)) \overset{P_\theta\text{ п.н.}}{=} 0
  \]
\end{definition}

\begin{theorem}
  Лемана-Шеффе.

  Пусть $T$ -- полная достаточная статистика для $\{P_\theta,\, \theta \in \Theta\}$, $d(X)$ -- несмещённая для $\tau(\theta)$. Тогда $\phi = \mathbb{E}(d \:\vert\: T)$ -- несмещённая оценка с равномерно минимальной дисперсией для $\tau(\theta)$.

  Если $\mathbb{V}_\theta(\phi) < +\infty$, то $\phi$ -- оптимальная оценка.
\end{theorem}

\begin{proof}
  Очевидно, что $\phi$ несмещённая по той же логике, что и в теореме Колмогорова-Блекуэлла-Рао (свойство УМО).
  
  Пусть $\tilde{d}$ -- другая несмещённая оценка. Тогда улучшим её $\tilde{\phi} = \mathbb{E}(\tilde{d} \:\vert\: T)$ не хуже $d$ по (\ref{KBR}) и несмещённая.
  
  Имеем 
  \[
    \mathbb{E}_\theta(\phi(T) - \tilde{\phi}(T)) = \tau(\theta) - \tau(\theta) = 0
  \]
  то есть для $h = \phi - \tilde{\phi}$ имеем $\forall \theta :\: \mathbb{E}_\theta h(T) = 0$. В силу полноты $T$ получаем, что $\forall \alpha :\: h(T) \overset{P_\theta \text{-п.н. }}{=} 0$. То есть наши оценки на самом деле равны почти наверное.

  То есть любая несмещённая оценка, пройдя процедуру улучшения с помощью $T$ почти наверное превращается в $\phi$.

  То есть 
  \[
    \forall \tilde{d} :\: \mathbb{V}_\theta(\tilde{d}) \geq \mathbb{V}_\theta(\tilde{\phi}) = \mathbb{V}_\theta(\phi)
  \]
  Пусть $\mathbb{V}_\theta(\phi) < +\infty$, теперь предполагаем, что неравенство на самом деле равенство. 

  Это по (\ref{UMO_uneq}) означает, что $\tilde{d}$ уже была $T$-измеримой, то есть $\tilde{d} = \tilde{\phi}$
\end{proof}

\begin{theorem}
  Об экспоненциальном семействе.

  Пусть $X_i$ -- выборка из экспоненциального семейства. Если область значений векторной функции 
  \[
    \overline{a}(\theta) = (a_1(\theta),\,\cdots,\,a_k(\theta)),\, \theta \in \Theta
  \]
  содержит $k$-мерный параллелепипед, то
  \[
    T(X) = (\sum_{i = 1}^nT_1(X_i),\,\cdots,\,\sum_{i = 1}^nT_k(X_i))
  \]
  является полной и достаточной для $\theta$.
\end{theorem}

\begin{note}
  Алгоритм поиска оптимальной оценки:
  \begin{enumerate}
    \item Ищем достаточную статистику $T$
    \item Проверяем на полноту
    \item Если полная, то решаем уравнение 
    \[
      \forall \theta \in \Theta :\: \mathbb{E}_\theta g(T(X)) = \tau(\theta)
    \]
  \end{enumerate}
\end{note}

\section{Доверительные интервалы}
\begin{definition}
    Пара статистик $(T_1(X),\,T_2(X))$ называются \textbf{доверительным интервалом} уровня доверия $\gamma$ для параметра $\theta$, если 
    \[
      \forall \theta \in \Theta :\: P_\theta(T_1(X) \leq \theta \leq T_2(X)) \geq \gamma
    \]
    Если равество достигается при всех $\theta \in \Theta$, то доверительный интервал называют \textbf{точным}.
\end{definition}

\begin{definition}
  Множество $S(X) \subset \Theta$ называют \textbf{доверительным множеством} уровня доверия $\gamma$ для параметра $\theta$, если
  \[
    \forall \theta \in \Theta :\: P_\theta(\theta \in S(X)) \geq \gamma
  \]
\end{definition}

\begin{note}
  Метод центральных статистик.

  Пусть $\exists$ известная одномерная функция $G(x,\, \theta)$, такая что её распределение не зависит от параметра $\theta$. Такая функция $G$ называется \textbf{центральной статистикой}.

  Пусть $\gamma_1,\,\gamma_2 \in (0,\,1)$ таковы, что $\gamma_2 - \gamma_1 = \gamma$ и при $i = 1,\,2 :\: \exists g_i$ -- $\gamma_i$-квантиль $G(X,\, \theta)$.
  
  Тогда
  \[
    \forall \theta \in \Theta :\: P_\theta(g_1 \leq G(X,\, \theta) \leq g_2) \geq \gamma_2 - \gamma_1 = \gamma
  \]
  Введём обозначение
  \[
    S(X) = \{\theta \in \Theta \:\vert\: g_1 \leq G(X,\,\theta) \leq g_2\}
  \]
  для $\forall \theta \in \Theta$ имеем
  \[
    P_\theta(\theta \in S(X)) \geq \gamma
  \]
  то есть $S(X)$ -- доверительное множество уровня доверия $\gamma$.

  Докажем корректность данного метода, заметим, что
  \[
    g_i = F^{-1}(\gamma_i) = \inf\{x :\: F(x) \geq \gamma_i\} \Rightarrow F(g_i) \geq \gamma_i
  \]
  и при $t < g_i :\: F(t) < \gamma_i \Rightarrow$ устремляя $t \to g_i - 0$ получим $F(g_i - 0) \leq \gamma_i$. Тогда
  \[
    P_\theta(g_1 \leq G(X,\, \theta) \leq g_2) = P_\theta(G(X,\,\theta) \leq g_2) - P_\theta(G(X,\,\theta) < g_1) \geq \gamma
  \]
  что и требовалось.
\end{note}

Для поиска центральных статистик в общем случае можно пользоваться следующей леммой:
\begin{lemma}
  $X_1,\,\cdots,\,X_n$ -- независимые одинаково распределённые с непрерывной функцией распределения $F(x)$. Тогда
  \[
    G(X_1,\,\cdots,\,X_n) = -\sum_{i = 1}^n\ln F(X_i) \sim \Gamma(1,\,n)
  \]
\end{lemma}

\begin{proof}
  Заметим, что
  \[
    P(F(y) \leq x) = P(y \leq F^{-1}(x)) = F(F^{-1}(x)) = x,\, x \in [0,\, 1] \Rightarrow F(x) \sim U[0,\,1]
  \]
  Несложным упражнением докажите, что $-\ln U[0,\,1] \sim \exp(1)$ и тогда из свойства аддитивности экспоненциальных распределений, утверждение леммы станет очевидным.
\end{proof}

\begin{definition}
  Пусть $\{X_n\}_{n = 1}^\infty$ -- выборка неограниченного размера из неизвестного распределения $P \in \{P_\theta,\, \theta \in \Theta\}$. Последовательность пар статистик 
  \[
    (T_n^{(1)}(X_1,\,\cdots,\,X_n),\, T_n^{(2)}(X_1,\,\cdots,\,X_n))
  \]
  называют \textbf{асимптотическим доверительным интервалом} уровня доверия $\gamma$ для $\theta$, если
  \[
    \forall \theta \in \Theta :\: \underline{\lim}_{n \to +\infty} P_\theta(T_n^{(1)}(X_1,\,\cdots,\,X_n) \leq \theta \leq T_n^{(2)}(X_1,\,\cdots,\,X_n)) \geq \gamma
  \]
  Если неравенство выше заменить на равенство (при условии, что нижний предел равен верхнему), то асимптотический доверительный интервал называют \textbf{точным}.
\end{definition}

\begin{note}
  Построение асимптотических интервалов.

  Пусть $\hat{\theta}_n(X_1,\,\cdots,\,X_n)$ -- асимптотически нормальная оценка $\theta$ с асимптотической дисперсией $\sigma^2(\theta) > 0$, то есть
  \[
    \forall \theta \in \Theta :\: \sqrt{n}(\hat{\theta}_n - \theta) \overset{d}{\to} \mathcal{N}(0,\, \sigma^2(\theta))
  \]
  Тогда при условии $\sigma(\theta)$ -- непрерывна, будем иметь, что $\hat{\theta}_n \overset{P_\theta}{\to} \theta$. Значит
  \[
    \sqrt{n}\frac{\hat{\theta}_n - \theta}{\sigma(\hat{\theta}_n)} = \sqrt{n}\frac{\hat{\theta}_n - \theta}{\sigma(\theta)} \cdot\frac{\sigma(\theta)}{\sigma(\hat{\theta}_n)} \overset{d_\theta}{\to} \sqrt{n}\frac{\hat{\theta}_n - \theta}{\sigma(\theta)} \sim \mathcal{N}(0,\, 1)
  \]
  по лемме Слуцкого.

  И теперь не сложно догадаться, как будет выглядеть доверительный интервал
  \[
    P_\theta\left(\sqrt{n}\left\vert\frac{\hat{\theta}_n - \theta}{\sigma(\hat{\theta}_n)}\right\vert < u_{\frac{1 + \gamma}{2}}\right) \overset{n \to +\infty}{\to} \gamma
  \]
\end{note}

\section{Метод максимального правдоподобия}
\begin{definition}
  Пусть $X$ -- наблюдения с функцией правдоподобия $f_\theta(X)$, тогда оценкой параметра $\theta$ по \textbf{методу максимального правдоподобия} (ОМП) называется такая статистика $\hat{\theta}(X)$, что
  \[
    \hat{\theta}(X) = \text{argmax}_{\theta \in \Theta}f_\theta(X)
  \]
\end{definition}

\begin{note}
  Будем использовать новые условия регулярности:
  \begin{enumerate}
    \item $\{P_\theta,\, \theta \in \Theta\}$ -- параметрическое семейство доминируемое относительно меры $\mu$, причём $P_{\theta_1} \neq P_{\theta_2}$ при $\theta_1 \neq \theta_2$ и для $\forall \theta$ определена $P_\theta(X)$ -- плотность $P_\theta$ относительно меры $\mu$.
    \item $A = \{x \in X :\: P_\theta(x) > 0\}$ не зависит от $\theta$
    \item Наблюдение $X$ есть выборка из неизвестного распределения $p \in \{P_\theta,\, \theta \in \Theta\}$
    \item $\Theta$ -- открытый интервал в $\mathbb{R}$ (возможно бесконечный)
    \item Функция $p_\theta(x)$ непрерывно дифференцируемая по $\theta$ при всех $x \in A$.
    \item $p_\theta(x)$ трижды непрерывно дифференцируема по $\theta \: \forall x \in A$.
    \item Интеграл $\int_Ap_\theta(x)\mu(dx)$ трижды дифференцируем по $\theta$ под знаком интеграла.
    \item $\mathbb{E}(\frac{\partial}{\partial\theta}\ln p_\theta(X_1))^2 = i(\theta) \in (0,\,+\infty)$
    \item Выполняется
    \[
      \forall  \theta_0 \in \Theta \: \exists c > 0 \: \exists H(x) \: \forall \theta \in (\theta_0 - c,\, \theta_0 + c) \: \forall x \in A :\: \left\vert\frac{\partial^3}{\partial\theta^3}\ln p_\theta(x)\right\vert < H(x)
    \]
    причём $\mathbb{E}_{\theta_0}H(X_1) < +\infty$
  \end{enumerate}
\end{note}

\begin{theorem}
  Экстремальное свойство правдоподобия.
  
  Пусть выполнены условия регулярности 1-3. Тогда
  \[
    \forall \theta_0,\,\theta \in \Theta \: \theta_0 \neq \theta :\: P_{\theta_0}(f_{\theta_0}(X_1,\,\cdots,\,X_n) > f_\theta(X_1,\,\cdots,\,X_n)) \overset{n \to +\infty}{\to} 1
  \]
\end{theorem}

\begin{proof}
  Пусть $X_i \in A$. Заметим, что
  \[
    f_{\theta_0}(X_1,\,\cdots,\,X_n) > f_\theta(X_1,\,\cdots,\,X_n) \Leftrightarrow \frac{1}{n}\ln\frac{f_\theta(X_1,\,\cdots,\,X_n)}{f_{\theta_0}(X_1,\,\cdots,\,X_n)} < 0 \Leftrightarrow \frac{1}{n}\sum_{i = 1}^n\ln\frac{f_\theta(X_i)}{f_{\theta_0}(X_i)} < 0
  \]
  Хотим применить ЗБЧ, а для этого нужно
  \[
    \mathbb{E}_\theta\frac{f_\theta(X)}{f_{\theta_0}(X)} < 0
  \]
  то есть
  \begin{align*}
    \int_A\ln\frac{p_\theta(x)}{p_{\theta_0}(x)}p_{\theta_0}(x)dx = \int_A\ln\left(1 + \frac{p_\theta(x)}{p_{\theta_0}(x)} - 1\right)p_{\theta_0}(x)dx \leq\\
    \int_A\left(\frac{p_\theta(x)}{p_{\theta_0}(x)} - 1\right)p_{\theta_0}(x) = \int_Ap_\theta(x)dx - \int_Ap_{\theta_0}(x)dx = 1 - 1 = 0 
  \end{align*}
  Причём равенство выполняется тогда и только тогда, когда
  \[
    \mu(x \in A :\: p_\theta(x) \neq p_{\theta_0}(x)) = 0
  \]
  что противоречит первому условию регулярности.
\end{proof}

\begin{corollary}
  Если $\Theta$ конечно, то ОМП существует, единственная с вероятностью $\to 1$, и состоятельная
\end{corollary}

\begin{proof}
  Максимум $f_\theta(X_1,\,\cdots,\,X_n)$ с ростом $n$ будет достигаться на истинном значении $\theta$ с вероятностью $\to 1$.

  Почему вообще оценка измеримая?
  \[
    \{x :\: \text{argmax}\cdots = \theta_2\} = \{f_{\theta_1(x)} < f_{\theta_2}(x)\}\cap\{f_{\theta_3}(x) < f_{\theta_2}(x)\}\cap\cdots
  \]
  то есть конечное пересечение измеримых множеств.

  Как проверить, что $\exists$ измеримая версия ОМП, если максимум может достигаться при разных $\theta$?

  Если кандидатов несколько, то выберем с наименьшими номером. Тогда если введём $c_{i < j} = \{x :\: f_{\theta_i}(x) < f_{\theta_j}(x)\}$, то
  \[
    \{\hat{\theta} = \theta_1\} = \cap_{j \neq 1}c_{j \leq 1};\;\;\; \{\hat{\theta} = \theta_2\} = c_{2 > 1} \cap (\bigcap_{j \geq 3}c_{j \leq 2});\;\;\;\;\cdots
  \]
\end{proof}

\begin{definition}
  \textbf{Уравнением правдоподобия} называют
  \[
    \frac{\partial\ln f_\theta}{\partial\theta} = 0 \Leftrightarrow \frac{\partial f_\theta}{\partial\theta} = 0
  \]
\end{definition}

\begin{theorem}
  Аналог состоятельности ОМП.

  Пусть выполняются условия регулярности 1-5 и пусть элементы выборки имеют распределение $P_{\theta_0}$. Тогда $\exists$ отображение $\hat{\theta}_n(X_1,\,\cdots,\,X_n,\,\theta_0)$ со значениями в $\Theta$:
  \[
    (P_{\theta_0})^*(\{\theta_n \text{ не решение уравнения правдоподобия}\}) \overset{n \to +\infty}{\to} 0
  \]
  и
  \[
    \forall \varepsilon > 0 :\: (P_{\theta_0})^*(\{\vert\hat{\theta}_n - \theta_0\vert > \varepsilon\}) \overset{n \to +\infty}{\to} 0
  \]
\end{theorem}

\begin{proof}
  Определим $\hat{\theta}_n$, фиксируя $X_1,\,\cdots,\,X_n$ из множества $A$.

  Если у уравнения $\frac{\partial\ln f}{\partial\theta} = 0$ есть хотя бы $1$ корень, то возьмём ближайший корень к $\theta_0$ (в силу непрерывности $\frac{\partial f}{\partial \theta}$ такое возможно, так как предел последовательности корней сам является корнем).

  Если же у уравнения нет корней, то доопределим $\hat{\theta}_n := \theta_0$.

  Фиксируем $\varepsilon > 0$, что $[\theta_0 - \varepsilon,\, \theta_0 + \varepsilon] \subset \Theta$. Рассмотрим
  \[
    s_n(\theta_0,\, \varepsilon) = \{x :\: f_{\theta_0 - \varepsilon}(x_1,\,\cdots,\,x_n) < f_{\theta_0}(x_1,\,\cdots,\,x_n),\, f_{\theta_0}(\cdots) > f_{\theta_0 + \varepsilon}(\cdots)\}
  \]
  Но по предыдущей теореме мы можем сказать, что $P_{\theta_0}(s_n) \overset{n \to +\infty}{\to} 1$.

  Далее, $\forall x \in s_n \exists$ точка $\tilde{\theta}_n$, в которой $f_\theta$ имеет локальный максимум $\Rightarrow f'(\tilde{\theta}_n) = 0$, причём $\tilde{\theta}_n \in U_\varepsilon(\theta_0)$.

  А так как $\hat{\theta}_n$ -- ближайший к $\theta_0$ корень, то $\vert\hat{\theta}_n - \theta_0\vert < \varepsilon$.

  Тогда $\{\hat{\theta}_n \text{ - не решение уравнения}\} \subset \{\mathcal{X}^n \setminus s_n\}$, навесив внешнюю меру, то получим первое неравенство из теоремы.

  Теперь осталось заметить, что если у нас не выполненяется неравенство $\vert\hat{\theta}_n - \theta\vert > \varepsilon$, то мы точно не в $s_n$, а значит снова сможем оценить сверху мерой $(P_{\theta_0}^*)(\mathcal{X}^n \setminus s_n) \overset{n \to +\infty}{\to} 0$
\end{proof}

\begin{note}
  Почему это почти состоятельность? Не проверяли измеримость $\hat{\theta}_n$ и всё равно $\hat{\theta}_n$ зависит от $\theta_0$.

  Если корней несколько, то неясно, какой ближе к $\theta_0$.

  Не факт, что корень -- глобальный максимум.

  Корень существует не всегда.
\end{note}

\begin{corollary}
  Пусть выполняются условия регулярности 1-5 и $\forall n \: \forall X_1,\,\cdots,\,X_n :\: \exists!$ решение $\hat{\theta}_n(X_1,\,\cdots,\,X_n)$ уравнения правдоподобия и пусть оно является измеримой функцией от выборки.

  Тогда $\hat{\theta}_n$ -- состоятельная оценка $\theta$ и с вероятностью стремящейся к 1, $\hat{\theta}_n$ является ОМП.
\end{corollary}

\begin{proof}
  Первая часть теоремы следует из предыдущей.

  Как и ранее, с большой вероятностью выполняется
  \[
    f_{\theta_0 - \varepsilon}(x_1,\,\cdots,\,x_n) < f_{\theta_0}(x_1,\,\cdots,\,x_n),\, f_{\theta_0 + \varepsilon}(x_1,\,\cdots,\,x_n) < f_{\theta_0}(x_1,\,\cdots,\,x_n)
  \]
  На отрезке $[\theta_0 - \varepsilon,\, \theta_0 + \varepsilon]$ достигается максимум, это следует из непрерывности плотности, а это следует из предположения регулярности.

  Этот максимум достигается на внутренней точке отрезка, которую обозначим $\theta_n^*$, и в ней $\frac{\partial}{\partial\theta}f = 0$, так как корень единственный, то $\hat{\theta}_n = \theta^*_n$.

  То есть $\hat{\theta}_n$ -- локальный максимум, но пусть существует $\tilde{\theta}_n$, в которой значение $f$ не меньше, чем в $\hat{\theta}_n$. Но тогда между $\hat{\theta}_n,\, \tilde{\theta}_n$ будет точка, в которой занулится производная (локального минимума), что противоречит с единственностью корня уравнения правдоподобия.
\end{proof}

\section{Дополнительные свойства ОМП}
\begin{theorem}
  (б/д)

  В условиях регулярности 1-9 $\forall$ состоятельной последовательности оценок $\hat{\theta}_n$, являющихся решениями уравнения правдоподобия, выполняется
  \[
    \forall \theta \in \Theta :\: \sqrt{n}(\hat{\theta}_n - \theta) \overset{d_\theta}{\to} \mathcal{N}\left(0,\, \frac{1}{i(\theta)}\right)
  \]
\end{theorem}

\begin{theorem}
  Бахадура. (б/д)

  Пусть выполнены условия регулярности 1-9 и $\hat{\theta}_n(X_1,\,\cdots,\,X_n)$ -- асимптотически нормальная оценка $\theta$, причём $\sigma(\theta)$ непрерывна по $\theta$. Тогда 
  \[
    \forall \theta \in \Theta :\: \sigma^2(\theta) \geq \frac{1}{i(\theta)}
  \]
\end{theorem}

\begin{definition}
  Если $\sqrt{n}(\hat{\theta}_n(X_1,\,\cdots,\,X_n) - \theta) \overset{d_\theta}{\to} \mathcal{N}(0,\, \frac{1}{i(\theta)})$, то $\hat{\theta}_n$ называется \textbf{асимптотически эффективной} оценкой $\theta$.
\end{definition}

\begin{proposition}
  Пусть выполняются условия регулярности для неравенство Крамера-Рао, $\hat{\theta}(X)$ -- эффективная оценка и равенство из критерия эффективности для $\hat{\theta}(X)$ выполняется $\forall x \: \forall \theta$. Тогда $\hat{\theta}(X)$ -- ОМП. 
\end{proposition}

\begin{proof}
  Распишем это равенство
  \[
    \hat{\theta}(X) - \theta = \frac{1}{I_X(\theta)}\frac{\partial}{\partial\theta}\ln f_\theta(X)
  \]
  Тогда при $\theta < \hat{\theta}(X) :\: \frac{\partial}{\partial\theta}\ln f_\theta(X) > 0$, а при $\theta > \hat{\theta}(X) :\: \frac{\partial}{\partial\theta}\ln f_\theta(X) < 0 \Rightarrow \theta = \hat{\theta}(X)$ -- точка максимума $\ln f_\theta(X) \Rightarrow$ ОМП.
\end{proof}

\section{Линейная регрессионная модель}
В линейной модели наблюдения -- случайный вектор $X \in \mathbb{R}^n$, который представляется в виде $X = l + \varepsilon$, где $l$ неслучайный неизвестный вектор, а $\varepsilon$ -- случайный вектор (ошибка).

Про $\varepsilon$ известно, что $\mathbb{E}\varepsilon = 0$ и $\mathbb{V}\varepsilon = \sigma^2I_n$, где $I_n$ -- единичная матрица $n \times n,\, \sigma^2 > 0$.

Про $l$ известно, что $l \in L$ -- линейное подпространство $\mathbb{R}^n$.

Задача: оценить неизвестные параметры $l,\, \sigma^2$.

$L$ задано с помощью своего базиса $\{z_1,\,\cdots,\,z_k\}$ из вектор-столбцов, $\dim L = k$. Составим $Z = (z_1,\,\cdots,\,z_k)$, то есть $l = Z\theta$, где $\theta$ -- неизвестные координаты в базисе $z_1,\,\cdots,\,z_k$.

То есть задача свелась к оценке $\theta,\, \sigma^2$, где $\theta \in \mathbb{R}^k$.

\begin{definition}
  $\hat{\theta}(X) = \text{argmin}_\theta\|X - Z\theta\|^2$ называется \textbf{оценкой наименьших квадратов} для $\theta$.
\end{definition}

\begin{lemma}
  Решением задачи выше является
  \[
    \hat{\theta} = (Z^TZ)^{-1}Z^TX
  \]
\end{lemma}

\begin{proof}
  Вначале раскроем скалярку
  \[
    \|X - Z\theta\|^2 = \langle X - Z\theta,\, X - Z\theta\rangle = X^TX - 2X^TZ\theta + \theta^TZ^TZ\theta
  \]
  Функция минимальна в точке, где частные производные равны нулю.

  Дифференцируем по $\theta_i \Rightarrow$
  \[
    -2(X^TZ)_i + 2(\theta^TZ^TZ)_i = 0
  \]
  Это должно выполняться для всех координат, то есть
  \[
    X^TZ = \theta^TZ^TZ \Rightarrow Z^TX = Z^TZ\theta \Rightarrow \hat{\theta} = (Z^TZ)^{-1}Z^TX
  \]
\end{proof}

\begin{proposition}
  Данная оценка обладает следующими свойствами:
  \[
    \mathbb{E}\hat{\theta} = \theta;\;\;\;\; \mathbb{V}\hat{\theta} = \sigma^2(Z^TZ)^{-1}
  \]
\end{proposition}

\begin{proof}
  Распишем матож:
  \[
    \mathbb{E}\hat{\theta} = \mathbb{E}(Z^TZ)^{-1}Z^TX = (Z^TZ)^{-1}Z^T\mathbb{E}X
  \]
  Но мы знаем, что $X = Z\theta + \varepsilon \Rightarrow \mathbb{E}X = Z\Theta \Rightarrow$ всё кроме $\theta$ сократится и доказали.

  Теперь дисперсия:
  \[
    \mathbb{V}\hat{\theta} = \mathbb{V}(Z^TZ)^{-1}Z^TX = (Z^TZ)^{-1}Z^T\mathbb{V}X((Z^TZ)^{-1}Z^T)^T = \sigma^2I_n(Z^TZ)^{-1}
  \]
\end{proof}

\begin{theorem}
  (б/д)

  Пусть $t = T\theta$ -- линейная вектор-функция от $\theta,\, T \in \text{Mat}_{m \times k}$. Тогда оценка $\hat{t} = T\hat{\theta}$ является оптимальной оценкой $t$ в классе линейных несмещённых оценок.
\end{theorem}

\begin{lemma}
  \[
    \mathbb{E}\|X - Z\theta\|^2 = (n - k)\sigma^2
  \]
\end{lemma}

\begin{proof}
  Так как $\mathbb{E}(X - Z\hat{\theta}) = 0$, то
  \[
    \mathbb{E}\|X - Z\theta\|^2 = \text{tr}\mathbb{V}(X - Z\hat{\theta})
  \]
  Распишем ковариационную матрицу:
  \begin{align*}
    \mathbb{V}(X - Z\hat{\theta}) = \mathbb{V}[(I_n - Z(Z^TZ)^{-1}Z^T)X]=(I_n - A)\mathbb{V}X(I_n - A)^T =\\
    (I_n - A)\mathbb{V}X(I_n - A) = \sigma^2(I_n - 2A + A^2) = \sigma^2(I_n - A)
  \end{align*}
  Перейдём обратно к числам:
  \begin{align*}
    \mathbb{E}\|X - Z\theta\|^2 = \sigma^2\text{tr}(I_n - A) = \sigma^2(\text{tr}I_n - \text{tr}A) =\\
    \sigma^2(n -  \text{tr}I_k) = \sigma^2(n - k)
  \end{align*}
\end{proof}

\begin{corollary}
  \begin{itemize}
    \item $X - Z\hat{\theta} = \text{proj}_{L^\bot}X$
    \item $\frac{\|X - Z\theta\|^2}{n - k} = \frac{\|\text{proj}_{L^\bot}X\|^2}{n - k}$ -- несмещённая оценка $\sigma^2$.
  \end{itemize}
\end{corollary}

\begin{proof}
  Из линала помним, что
  \[
    X = \text{proj}_LX + \text{proj}_{L^\bot}X
  \]
  Но по определению оценки $\text{proj}_LX = Z\hat{\theta}$.

  Второй факт следует из предыдущей леммы.
\end{proof}

\section{Гауссовская линейная модель}
Если в линейной регрессионной модели $\varepsilon \sim \mathcal{N}(0,\, \sigma^2E) \Rightarrow$ модель называется \textbf{гаусовской линейной} моделью.

\begin{note}
  $\chi$-квадрат распределением с $k$ степенями свободы называют
  \[
    \chi^2_k = \Gamma\left(\frac{1}{2},\,\frac{k}{2}\right) \overset{d}{=} \xi_1^2 + \cdots + \xi_k^2
  \]
  где $\xi_i$ -- независимые стандартные нормальные.
\end{note}

\begin{proposition}
  Статистика $S(X) = (\text{proj}_LX,\, \|\text{proj}_{L^\bot}X\|^2)$ является достаточной для $(l,\,\sigma^2)$
\end{proposition}

\begin{proof}
  Будем искать достаточную статистику с помощью критерия факторизации, для этого выпишем правдоподобие
  \[
    p(X) = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^ne^{-\frac{\sum_{i = 1}^n(X_i - l_i)^2}{2\sigma^2}}
  \]
  Заметим, что $\sum_{i = 1}^n(X_i - l_i)^2 = \|X - l\|^2$, применим теорему Пифагора:
  \[
    \|X - l\|^2 = \|\text{proj}_LX - \text{proj}_Ll\|^2 + \|\text{proj}_{L^\bot}X - \text{proj}_{L^\bot}l\|^2 \overset{l \in L}{=} \|\text{proj}_LX - l\|^2 + \|\text{proj}_{L^\bot}X\|^2
  \]
  То есть
  \[
    p(X) = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n\exp\left(-\frac{1}{2\sigma^2}(\|\text{proj}_LX - l\|^2 + \|\text{proj}_{L^\bot}X\|^2)\right)
  \]
  Видно, что мы привели правдоподобие к виду, необходимому для применения критерия Неймана-Фишера $\Rightarrow$ рассматриваемые статистики действительно достаточные.
\end{proof}

\begin{theorem}
  (б/д)
  
  $S(X)$ -- полная статистика. 
\end{theorem}

\begin{corollary}
  \begin{itemize}
    \item $\hat{\theta}$ -- оптимальная оценка для $\theta$
    \item $Z\hat{\theta}$ -- оптимальная оценка для $l$
    \item $\frac{1}{n - k}\|X - Z\hat{\theta}\|^2$ -- оптимальная оценка для $\sigma^2$
  \end{itemize}
\end{corollary}

\begin{proof}
  Несмещённость всех оценок очевидна из предыдущих рассуждений.

  Покажем, что все они являются функциями от полных достаточных статистик:  
  \[
    Z\hat{\theta} = \text{proj}_LX;\;\;\;\; \hat{\theta} = (Z^TZ)^{-1}Z^T\text{proj}_LX;\;\;\;\; \frac{1}{n - k}\|X - Z\hat{\theta}\|^2 = \frac{1}{n - k}\|\text{proj}_{L^\bot}X\|^2
  \]
\end{proof}

\begin{proposition}
  В гаусовской линейной модели $\hat{\theta}$ и $X - Z\hat{\theta}$ независимы, причём
  \[
    \frac{1}{\sigma^2}\|Z\hat{\theta} - Z\theta\|^2 \sim \chi^2_k;\;\;\;\; \frac{1}{\sigma^2}\|X - Z\hat{\theta}\|^2 \sim \chi^2_{n - k};\;\;\;\; \hat{\theta} \sim \mathcal{N}(\theta,\, \sigma^2(Z^TZ)^{-1})
  \]
\end{proposition}

\begin{proof}
  Знаем, что
  \[
    \begin{cases}
      Z\hat{\theta} = \text{proj}_LX\\
      X - Z\hat{\theta} = \text{proj}_{L^\bot}X
    \end{cases}
  \]
  Согласно теореме об ортогональном разложении гауссовские вектора $Z\hat{\theta},\, X - Z\hat{\theta}$ независимы, причём
  \[
    \frac{1}{\sigma^2}\|Z\hat{\theta} - \mathbb{E}(Z\hat{\theta})\|^2 = \frac{1}{\sigma^2}\|Z\hat{\theta} - Z\theta\|^2 \sim \chi^2_k
  \]
  И для другого
  \[
    \frac{1}{\sigma^2}\|X - Z\hat{\theta} - \mathbb{E}(X - Z\hat{\theta})\|^2 = \frac{1}{\sigma^2}\|X - Z\hat{\theta}\|^2 \sim \chi^2_{n - k}
  \]
  Мы знаем, что $Z\hat{\theta},\, X - Z\hat{\theta}$ независимы, но по выкладкам выше мы знаем, что $\hat{\theta}$ -- линейная функция от $Z\hat{\theta}$, поэтому искомая независимость существует.

  Осталось заметить, что $\hat{\theta}$ -- гаусовская функция, как линейная функция от гаусовского $X$, причём её матожидание и дисперсия считались выше, поэтому её распределение, очевидно, $\mathcal{N}(\theta,\, \sigma^2(Z^TZ)^{-1})$
\end{proof}

\section{Безумные распределения и их свойства}
\begin{definition}
  Пусть $\xi \sim \mathcal{N}(0,\,1),\, \eta \sim \chi^2_k$, причём $\xi$ и $\eta$ независимые. Тогда случайная величина
  \[
    \zeta = \frac{\xi}{\sqrt{\frac{\eta}{k}}}
  \]
  имеет \textbf{распределение Стьюдента} с $k$ степенями свободы, обозначение $\zeta \sim T_k$.
\end{definition}

\begin{lemma}
  Свойства распределения Стьюдента:
  \begin{enumerate}
    \item $\zeta \sim T_k \Rightarrow -\zeta \sim T_k$
    \item $T_1 \sim \text{Cauchy},\, p(x) = \frac{1}{\pi(1 + x^2)}$
    \item $\zeta_k \sim T_k \Rightarrow \zeta_k \overset{d}{\to} \mathcal{N}(0,\,1)$
  \end{enumerate}
\end{lemma}

\begin{definition}
  Пусть $\xi \sim \chi^2_k,\, \eta \sim \chi^2_m$, причём они независимы. Тогда случайная величина
  \[
    \frac{\frac{\xi}{k}}{\frac{\eta}{m}} \sim F_{k,\,m}
  \]
  имеет \textbf{распределение Фишера} с параметрами $k,\,m$.
\end{definition}

\begin{lemma}
  Свойства распределения Фишера:
  \begin{enumerate}
    \item $\xi \sim T_m \Rightarrow \xi^2 \sim F_{1,\,m}$
    \item $\xi \sim F_{k,\,m} \Rightarrow \frac{1}{\xi} \sim F_{m,\,k}$
    \item Если $k$ фиксированно и $\xi_m \sim F_{k,\,m} \Rightarrow k\xi_m \overset{d}{\to} \chi^2_k;\; m \to +\infty$
    \item $\xi_{k,\,m} \sim F_{k,\,m} \Rightarrow \xi_{k,\,m} \overset{d}{\to} \xi \equiv 1;\; k,\,m \to +\infty$
  \end{enumerate}
\end{lemma}

\begin{theorem}
  Об ортогональном разложении. (б/д)

  Пусть $(X_1,\,\cdots,\,X_n) \sim \mathcal{N}(l,\,\sigma^2I_n)$ и $L_1,\,\cdots,\,L_r$ -- попарно ортогональные подпространства $\mathbb{R}^n$, причём 
  \[
    L_1\oplus\cdots\oplus L_r = \mathbb{R}^n
  \]
  Тогда $Y_i := \text{proj}_{L_i}X,\, i = \overline{1,\,r}$ -- независимые в совокупности нормальные случайные векторы, причём $\mathbb{E}Y_i = \text{proj}_{L_i}l$ и
  \[
    \frac{1}{\sigma^2}\|Y_i - \mathbb{E}Y_i\|^2 \sim \chi^2_{\dim L_i}
  \]
\end{theorem}

\begin{note}
  Доверительные интервалы для параметров гаусовской линейной модели:
  \begin{enumerate}
    \item Доверительный интервал для $\sigma^2$:
    \[
      \frac{1}{\sigma^2}\|X - Z\hat{\theta}\|^2 \sim \chi^2_{n - k}
    \]
    и $u_{1 - \gamma}$ -- квантиль $\chi^2_{n - k}$. Тогда
    \[
      \gamma = P\left(\frac{1}{\sigma^2}\|X - Z\hat{\theta}\|^2 > u_{1 - \gamma}\right) = P\left(\sigma^2 \in \left(0,\, \frac{\|X - Z\hat{\theta}\|^2}{u_{1 - \gamma}}\right)\right)
    \]
    \item Доверительный интервал для $\theta_i$
    
    Мы знаем распределение вектора $\hat{\theta} \sim \mathcal{N}(\theta,\, \sigma^2(Z^TZ)^{-1})$, пусть $A := (Z^TZ)^{-1}$. Тогда компонента имеет распределение $\hat{\theta}_i \sim \mathcal{N}(\theta_i,\, \sigma^2A_{ii})$.

    Сейчас мы знаем, что
    \[
      \begin{cases}
        \frac{\hat{\theta}_i - \theta_i}{\sqrt{\sigma^2A_{ii}}} \sim \mathcal{N}(0,\,1)\\
        \frac{1}{\sigma^2}\|X - Z\hat{\theta}\|^2 \sim \chi^2_{n - k}  
      \end{cases}
    \]
    Причём эти случайные величины независимы (доказали выше), значит
    \[
      \sqrt{\frac{n - k}{A_{ii}}}\frac{\hat{\theta}_i - \theta_i}{\|X - Z\hat{\theta}\|} \sim T_{n - k}
    \]
    Теперь мы можем написать доверительный интервал, используя табличку квантилей распределения Стьюдента.
    \item Доверительный интервал для $\theta$:
    
    Опять знаем распределения следующих независимых случайных величин:
    \[
      \begin{cases}
        \frac{1}{\sigma^2}\|Z\hat{\theta} - Z\theta\|^2 \sim \chi^2_k\\
        \frac{1}{\sigma^2}\|X - Z\hat{\theta}\|^2 \sim \chi^2_{n - k}
      \end{cases}
    \]
    Тогда
    \[
      \frac{n - k}{k}\frac{\|Z\hat{\theta} - Z\theta\|^2}{\|X - Z\hat{\theta}\|^2} \sim F_{k,\, n-k}
    \]
    Используя эту случайную величину мы можем построить доверительный эллипсоид для $\theta$ (так как это многомерная величина).
  \end{enumerate}
\end{note}

\section{Гипотезы}
Пусть наблюдение $X$ имеет неизвестное распределение $P \in \mathcal{P}$, где $\mathcal{P}$ -- некоторое семейство распределений.

\begin{definition}
  \textbf{Статистическая гипотеза} -- это предположение вида $P \in \mathcal{P}_0,\, \mathcal{P}_0 \subset \mathcal{P}$ -- подмножество распределений.

  Обозначение $H_0 :\: P \in \mathcal{P}_0$ -- гипотеза $H_0$.

  Текущая рассматриваемая гипотеза называется \textbf{основной}.
\end{definition}

Задача: по наблюдению $X$ либо принять $H_0$ (тогда мы сузим класс класс распределений с $\mathcal{P}$ до $\mathcal{P}_0$), либо отвергнем.

В последнем случае мы переходим к рассмотрению альтернативы (если она есть): $H_1 :\: P \in \mathcal{P}_1,\, \mathcal{P}_1 = \mathcal{P} \setminus \mathcal{P}_0$

\begin{definition}
  Пусть $X$ принимает значения в выборочном пространстве $\mathcal{X}$, а $S \subset \mathcal{X}$ -- некоторое подмножество. Если правило принятия $H_0$ выглядит следующим образом:
  \[
    H_0 \text{ отвергается} \Leftrightarrow X \in S
  \]
  то $S$ называют \textbf{критическим множеством}, то есть критерием для проверки гипотезы $H_0$.
\end{definition}

\begin{definition}
  \textbf{Ошибка первого рода} -- отвергли $H_0$, когда она верна.

  \textbf{Ошибка второго рода} -- приняли $H_0$, когда она неверна.
\end{definition}

Будем выбирать $S$ так, что вероятность ошибки 1 рода была меньше заранее выбранного $\varepsilon$, а вероятность ошибки 2 рода сделаем как можно меньше

\begin{definition}
  Пусть $S$ -- критерий для проверки гипотезы $H_0 :\: \mathcal{P}_0$. Функция $\beta(Q,\,S) = Q(X \in S),\, Q \in \mathcal{P}$ называется функцией \textbf{мощности критерия} $S$.
\end{definition}

\begin{definition}
  Если для критерия $S$ выполнено неравенство 
  \[
    \forall Q \in \mathcal{P}_0 :\: \beta(Q,\, S) \leq \varepsilon
  \]
  то говорят, что $S$ \textbf{имеет уровень значимости} $\varepsilon$.
\end{definition}

\begin{definition}
  Минимальный уровень значимости 
  \[
    \alpha(S) = \sup_{Q \in \mathcal{P}_0}\beta(Q,\, S)
  \]
  называется \textbf{мощностью критерия}.
\end{definition}

\begin{definition}
  Критерий $S$ называется \textbf{несмещённым}, если
  \[
    \sup_{Q \in \mathcal{P}_0}\beta(Q,\,S) \leq \inf_{Q \in \mathcal{P}_1}\beta(Q,\, S)
  \]
\end{definition}

\begin{definition}
  Если $X = (X_1,\,\cdots,\,X_n)$ -- выборка растущего размера, то критерий $S_n$ (точнее, последовательность критериев) называется \textbf{состоятельной}, если
  \[
    \forall Q \in \mathcal{P}_1 :\: \beta(Q,\, S_n) \overset{n \to +\infty}{\to} 1  
  \]
\end{definition}

\begin{definition}
  Критерий $S$ уровня значимости $\varepsilon$ называется \textbf{более мощным}, чем критерий $R$ того же уровня значимости, если
  \[
    \forall Q \in \mathcal{P}_1 :\: \beta(Q,\,S) \geq \beta(Q,\, R)
  \]
  то есть вероятность ошибки 2го рода у $S$ равномерно меньше.
\end{definition}

\begin{definition}
  Критерий $S$ называется \textbf{равномерно наиболее мощным} критерием уровня значимости $\varepsilon$, если $\alpha(S) \leq \varepsilon$, и $S$ мощнее $\forall$ другого критерия $R$, который удовлетворяет условию $\alpha(R) \leq \varepsilon$
\end{definition}

\begin{definition}
  Гипотеза $H :\: P = P_0$, где $P_0$ -- известное распределение, называется \textbf{простой}.
\end{definition}

\section{Построение гипотез}
\begin{lemma}
  Неймана-Пирсона.

  Пусть для простой гипотезы мы выбрали критерий $S_\lambda := \{X :\: p_1(X) - \lambda p_0(X) \geq 0\}$ и пусть критерий $R$ удовлетворяет условию $P_0(X \in R) \leq P_0(X \in S_\lambda)$. Тогда
  \begin{align*}
    P_1(X \in R) \leq P_1(X \in S_\lambda)\\
    P_0(X \in S_\lambda) \leq P_1(X \in S_\lambda)
  \end{align*}
\end{lemma}

\begin{proof}
  Заметим, что
  \begin{align*}
    \mathbb{I}_R(p_1(X) - \lambda p_0(X)) \leq \mathbb{I}_R(X)(p_1(X) - \lambda p_0(X))\mathbb{I}_{\{x :\: p_1(X) - \lambda p_0(X) \geq 0\}}(X) =\\
    \mathbb{I}_R(X)\mathbb{I}_{S_\lambda}(X)(p_1(X) - \lambda p_0(X)) \leq (p_1(X) - \lambda p_0(X))\mathbb{I}_{S_\lambda}(X)
  \end{align*}
  Следовательно
  \begin{align*}
    P_1(X \in R) - \lambda P_0(X \in R) = \mathbb{E}_1\mathbb{I}_R(X) - \lambda\mathbb{E}_0\mathbb{I}_R(X) =\\
    \int_\mathcal{X}\mathbb{I}_R(X)(p_1 - \lambda p_0)(X)d\mu \leq \int_\mathcal{X}\mathbb{I}_{S_\lambda}(X)(p_1(X) - \lambda p_0(X))d\mu =\\
    P_1(X \in S_\lambda) - \lambda P_0(X \in S_\lambda)
  \end{align*}
  Значит
  \[
    P_1(X \in S_\lambda) - P_1(X \in R) \geq \lambda(P_0(X \in S_\lambda) - P_0(X \in R)) \geq 0
  \]
  Для доказательства второго факта предположим $\lambda \geq 1$:
  \[
    P_0(X \in S_\lambda) = \int_\mathcal{X}\mathbb{I}_{S_\lambda}p_0(X)d\mu \leq \int_\mathcal{X}\mathbb{I}_{S_\lambda}p_1(X)d\mu = P_1(X \in S_\lambda)
  \]
  Пусть теперь $\lambda < 1$, тогда $\forall x \in \overline{S_\lambda} :\: p_1(x) \leq p_0(x)$:
  \[
    P_1(X \in \overline{S_\lambda}) = \int_\mathcal{X}\mathbb{I}_{\overline{S_\lambda}}p_1(X)d\mu \leq \int_\mathcal{X}\mathbb{I}_{\overline{S_\lambda}}p_0(X)d\mu = P_0(X \in \overline{S_\lambda}) \Rightarrow P_0(X \in S_\lambda) \leq P_1(X \in S_\lambda)
  \]
\end{proof}

\begin{corollary}
  Если $\lambda > 0$ удовлетворяет условию $P_0(X \in S_\lambda) = \varepsilon \Rightarrow S_\lambda$ -- равномерно наиболее мощный критерий размера $\varepsilon$.
\end{corollary}

\begin{note}
  Для нахождения этого критерия необходимо решить уравнение
  \[
    \int_{\{x :\: \{x :\: p_1(X) - \lambda p_0(X) \geq 0\})\}}p_0(x)d\mu = \varepsilon
  \]
  В случае абсолютно непрерывных распределений, решение, как правило, есть.

  А в дискретном случае уравнение неразрешимо для многих $\varepsilon > 0$. Тогда можно взять $\varepsilon_0$, для которого оно разрешимо.
\end{note}

\begin{definition}
  Пусть семейство $\mathcal{P}$ параметризовано параметром $\theta \in \mathbb{R}$. Пусть $\mathcal{P}$ доминируемо относительно меры $\mu$, то есть $\exists$ функция правдоподобия $f_\theta(X)$.

  Семейство $\{p_\theta,\, \theta \in \Theta\}$ называется \textbf{семейством с монотонным отношением правдоподобия} по статистике $T(X)$ если $\forall \theta_0 < \theta_1$: $\frac{f_{\theta_1}(X)}{f_{\theta_0}(X)}$ является монотонной функцией от $T(X)$, причём тип монотонности один и тот же для всех $\theta_1 > \theta_0$ 
\end{definition}

\begin{theorem}
  О монотонном отношении правдоподобия. (б/д)

  Пусть даны гипотезы 
  \[
    H_0 :\: \theta \leq \theta_0;\;\;\;\; H_1 :\: \theta > \theta_0
  \]
  а семейство $\{p_\theta,\, \theta \in \Theta\}$ -- семейство с монотонным отношением правдоподобия, причём $\frac{f_{\theta_1}(X)}{f_{\theta_2}(X)}$ не убывает по $T(X)$ при $\theta_1 > \theta_2$.

  Тогда критерий $S_\varepsilon = \{T(X) \geq C_\varepsilon\}$ с условием $P_{\theta_0}(S_\varepsilon) = \varepsilon$ является равномерным наиболее мощным критерием уровня значимости $\varepsilon$ для проверки $H_0$ против $H_1$.
\end{theorem}

\begin{note}
  Двойственность доверительного оценивания и проверки гипотез.

  \begin{enumerate}
    \item Пусть $S(X)$ -- доверительная область уровня доверия $1 - \varepsilon$ для параметра $\theta \in \Theta$. Хотим проверить простую гипотезу $H_0 :\: \theta = \theta_0$. Рассмотрим $\tilde{S}(\theta) = \{x \in \mathcal{X} :\: \theta \not\in S(X)\}$. Тогда $\tilde{S}(\theta_0)$ -- критерий уровня значимости $\varepsilon$ для проверки $H_0$. Действительно:
    \[
      P_{\theta_0}(X \in \tilde{S}(\theta_0)) = P_{\theta_0}(\theta_0 \not\in S(X)) = 1 - P_{\theta_0}(\theta_0 \in S(X)) \leq \varepsilon
    \]
    \item Пусть, наоборот, нам дан критерий $S_{\theta_0}$ уровня значимости $\varepsilon$ для проверки $H_0 :\: \theta = \theta_0$. И пусть $S_{\theta_0}$ известен для всех $\theta_0 \in \Theta$. Рассмотрим $S(X) = \{\theta \in \Theta :\: X \not\in S_{\theta}\}$. Проверим, что это доверительное множество уровня доверия $1 - \varepsilon$. Для
    \[
      \forall \theta \in \Theta :\: P_\theta(\theta \in S(X)) = P_\theta(X \not\in S_\theta) = 1 - P_\theta(X \in S_\theta) \geq 1 - \varepsilon
    \]
  \end{enumerate}
\end{note}

\section{Проверка гипотез в гаусовской линейной модели}
Цель: построить критерий для проверки линейной гипотезы $H_0 :\: T\theta = t$, где $T \in \mathbb{R}^{m \times k},\, t \in \mathbb{R}^m,\, \text{rk}T = m \leq k$.

Знаем: $\hat{\theta} \sim \mathcal{N}(\theta,\, \sigma^2(Z^TZ)^{-1})$ и $\hat{t} = T\hat{\theta} \sim \mathcal{N}(T\theta,\, T\sigma^2(Z^TZ)^{-1}T) =: \mathcal{N}(T\theta,\, \sigma^2B)$.

Заметим, что матрица $B$ положительно определена и симметрична, тогда
\[
  \exists \sqrt{B} :\: \sqrt{B}\sqrt{B} = B,\, (\sqrt{B})^T = \sqrt{B}
\]
Тогда
\[
  \frac{1}{\sigma}(\sqrt{B})^{-1}(\hat{t} - T\theta) \sim \mathcal{N}(0,\, (\sqrt{B})^{-1}B(\sqrt{B}^{-1})^T) = N(0,\,I_m)
\]
А это значит
\[
  \chi^2_m \sim \|\frac{1}{\sigma}(\sqrt{B})^{-1}(\hat{t} - T\theta)\|^2 = (\frac{1}{\sigma}(\sqrt{B})^{-1}(\hat{t} - T\theta))^T(\frac{1}{\sigma}(\sqrt{B})^{-1}(\hat{t} - T\theta)) = \frac{1}{\sigma^2}(\hat{t} - T\theta)^TB^{-1}(\hat{t} - T\theta)  
\]
Обозначим последнее выражение за $Q_T$, рассмотрим статистику
\[
  \hat{Q}_T = (\hat{t} - t)^TB^{-1}(\hat{t} - t)
\]
так как $\hat{Q}_T$ выражается через $\hat{Q} \Rightarrow$ не зависит от $X - Z\hat{\theta}$.

Значит в условиях $H_0$:
\[
  \frac{\hat{Q}_T}{\|X - Z\theta\|^2}\frac{n - k}{m} \sim F_{m,\,n - k}
\]
Теперь можем сформулировать \textbf{F-критерий}:
\[
  \left\{\frac{(T\hat{\theta} - t)^T(T(Z^TZ)^{-1}T^T)^{-1}(T\hat{\theta} - t)}{\|X - Z\hat{\theta}\|^2}\frac{n - k}{m} > u_{1 - \gamma}\right\}
\]
где $u_{1 - \gamma}$ -- квантиль $F_{m,\, n - k}$ распределения.

\begin{example}
  Пример с двумя гауссовскими выборками, отличающимися сдвигом: проверка гипотезы об их
  однородности.

  Пусть $X_1,\,\cdots,\,X_n \sim \mathcal{N}(a_1,\, \sigma^2);\; Y_1,\,\cdots,\,Y_m \sim \mathcal{N}(a_2,\, \sigma^2)$. Построим F-критерий для $H_0 :\: a = b$, тогда
  \[
    W = \begin{pmatrix}
      X_1\\
      \vdots\\
      X_n\\
      Y_1\\
      \vdots\\
      Y_m
    \end{pmatrix} = \begin{pmatrix}
      a_1\\
      \vdots\\
      a_1\\
      a_2\\
      \vdots\\
      a_2
    \end{pmatrix} + \vec{\varepsilon} = Z\theta + \vec{\varepsilon}
  \]
  где 
  \[
    Z = \begin{pmatrix}
      1 & 0\\
      \vdots\\
      1 & 0\\
      0 & 1\\
      \vdots\\
      0 & 1
    \end{pmatrix},\, \theta = \begin{pmatrix}
      a_1\\
      a_2\\
    \end{pmatrix}
  \]
  Получили, что
  \[
    \hat{\theta} = (Z^TZ)^{-1}Z^TW = \begin{pmatrix}
      \overline{X}\\
      \overline{Y}
    \end{pmatrix}; T = \begin{pmatrix}
      1 & -1
    \end{pmatrix}; \hat{t} = \overline{X} - \overline{Y}; t = 0
  \]
  Ну и 
  \[
    T(Z^TZ)^{-1}T^T = \frac{1}{n} + \frac{1}{m}; \hat{Q}_T = \left(\frac{1}{n} + \frac{1}{m}\right)^{-1}(\overline{X} - \overline{Y})^2
  \]
  Дальше всё это подставляем в формулу и всё получается.......
\end{example}

\section{Критерий Пирсона, Колмогорова}
\begin{note}
  Критерий согласия Пирсона (критерий хи-квадрат).

  Пусть $X_1,\,\cdots,\,X_n$ -- i.i.d., причём $P(X_1 = a_i) = p_i,\, i =\overline{1,\,m};\; \sum_{i = 1}^m p_i = 1$.

  Положим $\nu_j = \sum_{i = 1}^n \mathbb{I}\{X_i = a_j\}$ -- количество осуществления исхода $a_j$. Очевидно, $\sum_{j = 1}^n \nu_j = n$. Нам неизвестен
  \[
    \vec{p} = \begin{pmatrix}
      p_1\\
      \vdots\\
      p_m
    \end{pmatrix}
  \]
  Хотим проверить гипотезу $H_0 :\: \vec{p} = \vec{p}_0$, где $\vec{p}_0$ принадлежит вероятностному симплексу.
\end{note}

\begin{definition}
  Статистикой \textbf{хи-квадрат Пирсона} называют
  \[
    \hat{\chi}^2_n = \sum_{j = 1}^m\frac{(\nu_j - np_j^0)^2}{np_j^0}
  \]
\end{definition}

\begin{theorem}
  Пирсона. Доказательство в отдельном билете.

  Если выполнена $H_0$, то 
  \[
    \hat{\chi}^2_n \overset{d}{\to} \chi^2_{m - 1},\, n \to +\infty
  \]
\end{theorem}

Пусть $u_{1 - \varepsilon}$ -- $(1 - \varepsilon)$-квантиль распределения $\chi^2_{m - 1}$. Если $\hat{\chi}^2_{n} > u_{1 - \varepsilon} \Rightarrow H_0$ отвергается.

\begin{lemma}
  Критерия Пирсона состоятелен против альтернативы $\vec{p} \neq \vec{p}_0$
\end{lemma}

\begin{proof}
  Пусть на самом деле $\vec{p} \neq \vec{p}_0$:
  \[
    \hat{\chi}^2_n = n\sum_{i = 1}^m\left(\frac{\nu_i}{n} - p_i^0\right)^2\frac{1}{p_i^0}
  \]
  По УЗБЧ имеем, что $\forall i :\: \frac{\nu_i}{n} = \frac{1}{n}\sum_{j = 1}^n\mathbb{I}\{X_j = a_i\} \overset{\text{п.н.}}{\to} P(X_j = a_i) = p_i \Rightarrow$ по теореме о наследовании сходимости:
  \[
    \sum_{j = 1}^m\left(\frac{\nu_j}{n} - p_j^0\right)\frac{1}{p_j^0} \overset{\text{п.н.}}{\to} \sum_{j = 1}^m(p_j - p_j^0)^2\frac{1}{p_j^0} > 0
  \]
  Значит $\hat{\chi}^2_n \to +\infty$ почти наверное при $n \to +\infty :\: P(\hat{\chi}^2_n > u_{1 - \varepsilon}) \to 1,\, n \to +\infty$. То есть ошибка второго рода стремится к нулю.
\end{proof}

\begin{note}
  Критерий согласия Колмогорова.

  Пусть $X = (X_1,\,\cdots,\,X_n)$ -- выборка из неизвестного распределения на $\mathbb{R}$ с непрерывной функцией распределения $F$.

  Вспомним, что
  \[
    D_n = \sup_{x \in \mathbb{R}}\vert F_n^*(x) - F(x)\vert \overset{\text{п.н.}}{\to} 0
  \]
  Где $D_n$ -- случайная величина.
\end{note}

\begin{theorem}
  б/д.

  \begin{enumerate}
    \item Распределение $D_n$ не зависит от вида $F$.
    \item $\sqrt{n}D_n \overset{d}{\to} \xi$, где $\xi$ имеет распределение Колмогорова, то есть
    \[
      P(\xi \leq z) = \sum_{j \in \mathbb{Z}}(-1)^je^{-2j^2z^2}\mathbb{I}\{z > 0\}
    \]
  \end{enumerate}
\end{theorem}

\begin{definition}
  Рассмотрим $H_0 :\: F = F_0$ -- непрерывная, \textbf{критерием Колмогорова} называют множество
  \[
    S = \{\sqrt{n}\sup_{x \in \mathbb{R}}\vert F_n^*(x) - F_0(x)\vert > k_{1 - \alpha}\}
  \]
  где $k_{1 - \alpha}$ -- $(1 - \alpha)$-квантиль распределения Колмогорова, применим при $n \geq 20$.
\end{definition}

\begin{note}
  Критерий Смирнова-фон Мизеса.

  Рассмотрим статистику
  \[
    \omega^2 = n\int_\mathbb{R}(F_n^*(x) - F_0(x))^2dF_0(x)
  \]
  при $H_0 :\: F = F_0$ -- непрерывной, распределение $\omega^2$ не зависит от вида $F_0$ и $\omega^2 \overset{d}{\to} $известному распределению.
\end{note}

\begin{definition}
  Критерий Пирсона, Колмогорова и $\omega^2$ называют \textbf{критериями согласия}, так как они проверяют гипотезу вида $H_0 :\: P = P_0$.
\end{definition}

\section{Доказательство теоремы Пирсона}
\begin{theorem}
  Пирсона.

  Если выполнена $H_0$, то 
  \[
    \hat{\chi}^2_n \overset{d}{\to} \chi^2_{m - 1},\, n \to +\infty
  \]
\end{theorem}

\begin{proof}
  Введём
  \[
    Y_j = \begin{pmatrix}
      \mathbb{I}\{X_j = a_1\}\\
      \vdots\\
      \mathbb{I}\{X_j = a_m\}
    \end{pmatrix},\, 1 \leq j \leq n
  \]
  Получается, $\{Y_j\}$ -- независимые одинаково распределённые случайные векторы. Очевидно, что
  \[
    \mathbb{E}Y_j = \begin{pmatrix}
      p_1\\
      \vdots\\
      p_m
    \end{pmatrix} = \begin{pmatrix}
      p_1^0\\
      \vdots\\
      p_m^0
    \end{pmatrix}
  \]
  Теперь посчитаем ковариацию:
  \[
    \text{cov}(\mathbb{I}\{X_j = a_i\},\, \mathbb{I}\{X_j = a_k\}) = \mathbb{E}\mathbb{I}\{X_j = a_i,\, X_j = a_k\} - p_i^0p_k^0 = \begin{cases}
      p_i^0 - (p_i^0)^2,\, i = k\\
      -p_i^0p_j^0,\, i \neq k
    \end{cases}
  \]
  Положим
  \[
    B := \begin{pmatrix}
      p_1^0 & 0 & \cdots & 0\\
      \vdots & \ddots & \cdots & \vdots\\
      0 & 0 & \cdots & p_n^0
    \end{pmatrix}
  \]
  Тогда $\mathbb{V}Y_j = B - p_0^Tp_0$. По многомерной ЦПТ 
  \[
    \frac{Y_1 + \cdots + Y_n}{\sqrt{n}} - \sqrt{n}p_0 \overset{d}{\to} \mathcal{N}(0,\, B - p_0^Tp_0)
  \]
  Заметим, что сумма $Y_i$ это
  \[
    Y_1 + \cdots + Y_n = \begin{pmatrix}
      \nu_1\\
      \vdots\\
      \nu_m
    \end{pmatrix} = \vec{\nu}
  \]
  Пусть $\xi_n := (\sqrt{B})^{-1}\sqrt{n}\left(\frac{\vec{\nu}}{n} - \vec{p}_0\right)$ тогда по теореме о наследовании сходимости
  \[
    \xi_n \overset{d}{\to} \mathcal{N}(0,\,(\sqrt{B})^{-1}(B - p_0p_0^T)(\sqrt{B})^{-1}) = \mathcal{N}(0,\,I_m - ZZ^T);\; Z = \begin{pmatrix}
      \sqrt{p_1^0}\\
      \vdots\\
      \sqrt{p_m^0}
    \end{pmatrix}
  \]
  Рассмотрим ортогональную матрицу $V \in \mathbb{R}^{m \times m}$, такую что её первая строка -- это $(\sqrt{p_1^0},\,\cdots,\,\sqrt{p_m^0})$
  
  Тогда по теореме о наследовании сходимости:
  \[
    V\xi_n \overset{d}{\to} V\mathcal{N}(0,\, I_m - ZZ^T) = \mathcal{N}(0,\, V(I_m - ZZ^T)V^T)
  \]
  Заметим, что 
  \[
    VZ = \begin{pmatrix}
      \sqrt{p_1^0} & \cdots & \sqrt{p_m^0}\\
      \vdots & \ddots & \vdots\\
      \vdots & \cdots & \ddots\\
    \end{pmatrix}
    \begin{pmatrix}
      \sqrt{p_1^0}\\
      \vdots\\
      \sqrt{p_m^0}
    \end{pmatrix} = \begin{pmatrix}
      1\\
      \vdots\\
      0
    \end{pmatrix}
  \]
  Тогда
  \[
    V(I_m - ZZ^T)V^T = I_m - (VZ)(VZ)^T = I_m - \begin{pmatrix}
      1 & 0 & 0 & 0\\
      0 & \cdots & 0 & 0\\
      \vdots & \ddots & \cdots & \vdots\\
      0 & 0 & 0 & 0
    \end{pmatrix} := \tilde{I}_m
  \]
  По теореме о наследовании сходимости
  \[
    \|V\xi_n\|^2 \overset{d}{\to} \|\mathcal{N}(0,\, \tilde{I}_m)\|^2 \overset{d}{=} \chi^2_{m - 1}
  \]
  Но $V$ ортогональна, значит $\|V\xi_n\|^2 = \|\xi_n\|^2 = \|(\sqrt{B})^{-1}\sqrt{n}\left(\frac{\nu}{n} - \vec{p}_0\right)\|^2 = \hat{\chi}^2_n$
\end{proof}

\section{Байесовские оценки}
Пусть $\Theta \subset \mathbb{R}^d$. В байесовском подходе параметр $\theta$ является случайной величиной (или вектором) с известным распределением $Q$ на множестве $\Theta$.

А именно, пусть $Q$ -- распределение на $(\Theta,\, \beta_\Theta)$ с плотностью $q(t)$. Рассмотрим вероятностное пространство $(\Theta \times\mathcal{X},\, \beta_\Theta\otimes\beta_\mathcal{X},\, \tilde{p})$, где $\tilde{p}$ -- мера с плотностью $f(t,\,x) = q(t)p_t(x)$. 

Тогда $(\theta,\, X)$ -- случайный вектор на этом вероятностном пространстве с плотностью $f(t,\,x)$ при таком подходе $p_t(x)$ -- условная плотность $X$ при условии, что $\theta = t$ фиксированная.

Рассмотрим $\theta(t) = t$ -- случайная величина на $(\Theta,\, \beta_\Theta,\, Q)$ и пусть $X$ -- случайная величина на $(\mathcal{X},\, \beta_\mathcal{X},\, p_t)$. Тогда $(\theta,\, X)$ -- случайный вектор на $(\Theta \times\mathcal{X},\, \beta_\Theta\otimes\beta_\mathcal{X},\, \tilde{p})$ с плотностью $f$, причём $q(t)$ -- плотность $\theta$, а $p_t(x)$ -- условная плотность $X$ при $\theta = t$

\begin{definition}
  Плотность $q(t)$ называется \textbf{априорной} плотностью параметра $\theta$, а условной плотностью $\theta$ относительно $x$ 
  \[
    q(t \:\vert\: x) = \frac{q(t)p_t(x)}{\int_\Theta q(u)p_u(x)du}
  \]
  называется \textbf{апостериорной} плотностью $\theta$.
\end{definition}

\begin{definition}
  Оценка $\hat{\theta}(X) = \int_\Theta tq(t \:\vert\: x)dt = \mathbb{E}_{\tilde{p}}(\theta \:\vert\: X)$ называется \textbf{байесовской} оценкой параметра $\theta$.
\end{definition}

\begin{theorem}
  Байесовская оценка является наилучшей оценкой в байесовском подходе с квадратичной функцией потерь.
\end{theorem}

\begin{proof}
  Пусть $\hat{\theta}(X)$ -- байесовская оценка $\theta$. Хотим минимизировать
  \[
    \int_\Theta R(\hat{\theta},\, \theta)q(\theta)d\theta = \int_\Theta\mathbb{E}_\theta(\hat{\theta} - \theta)^2q(\theta)d\theta = \int_\Theta\int_\mathcal{X}(\hat{\theta} - \theta)^2f(\theta,\,x)dxd\theta = \mathbb{E}_{\tilde{p}}(\hat{\theta}(X) - \theta)^2 
  \]
  А по теореме о наилучшем квадратичном прогнозе данное матожидание будет минимальным при $\mathbb{E}_{\tilde{p}}(\theta\:\vert\:X)$
\end{proof}

\section{Коэффициенты корреляции}
Теперь будем рассматривать выборки $X = (X_1,\,\cdots,\,X_n)$ и $Y = (Y_1,\,\cdots,\,Y_n)$ -- две выборки одинакового распределения. Нас интересует гипотеза о независимости $\vec{X},\, \vec{Y}$:
\[
  H_0 :\: F_{X,\, Y}(t,\,s) = F_X(t)F_Y(s)
\]
Пусть $\mathbb{E}X_1^2 < +\infty$ и $\mathbb{E}Y_1^2 < +\infty$.

\begin{definition}
  Величина
  \[
    \hat{\rho} = \frac{\sum_{i = 1}^n(X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum_{i = 1}^n(X_i - \overline{X})(Y_i - \overline{Y})}}
  \]
  называется \textbf{коэффициентом корреляции Пирсона}.
\end{definition}

\begin{proposition}
  Выполняется сходимость
  \[
    \hat{\rho} \overset{\text{п.н.}}{\to} \rho = \frac{\text{cov}(X,\,Y)}{\sqrt{\mathbb{V}X\mathbb{V}Y}}
  \]
\end{proposition}

\begin{proof}
  Вспомним свойство выборочной дисперсии:
  \[
    S_X^2 = \frac{1}{n}\sum_{i = 1}^n(X_i - \overline{X}) = \overline{X^2} - (\overline{X})^2
  \]
  По УЗБЧ:
  \[
    \begin{cases}
      \overline{X^2} \overset{\text{п.н.}}{\to} \mathbb{E}X_1^2\\
      \overline{X} \overset{\text{п.н.}}{\to} \mathbb{E}X_1
    \end{cases} \Rightarrow S_X^2 \to \mathbb{E}X_1^2 - (\mathbb{E}X_1)^2 = \mathbb{V}X
  \]
  То есть сходимость знаменателя доказали. Теперь числитель:
  \[
    \frac{1}{n}\sum_{i = 1}^n(X_i - \overline{X})(Y_i - \overline{Y}) = \frac{\sum_{i=1}^nX_iY_i}{n} - \overline{X}\frac{\sum_{i = 1}^nY_i}{n} - \overline{Y}\frac{\sum_{i = 1}^nX_i}{n} + \overline{X}\cdot\overline{Y} \overset{\text{п.н.}}{\to} \mathbb{E}X_1Y_1 - \mathbb{E}X_1\mathbb{E}Y_1
  \]
\end{proof}

\begin{theorem}
  Б/Д.

  Пусть $n > 2$. $X$ и $Y$ -- две независимые выборки, имеющие нормальное распределение. Тогда
  \[
    T := \frac{\hat{\rho}\sqrt{n - 2}}{\sqrt{1 - \hat{\rho}^2}} \sim T_{n - 2}
  \]
\end{theorem}

Тогда критерием будет $S = \{\vert T\vert > const\}$

\begin{note}
  Коэффициент корреляции Спирмана.

  Пусть $X_1,\,\cdots,\,X_n$ -- выборка из непрерывного распределения, упорядочим элементы выборки по возрастанию.
\end{note}

\begin{definition}
  Номера, которые получили элементы выборки при таком упорядочивании, называются \textbf{рангами}
  \[
    R(X_i) \text{ - номер }X_i\text{ в вариационном ряду}
  \]
\end{definition}

\begin{note}
  Если $(r_1,\,\cdots,\,r_n) \in s_n$ -- перестановка, то
  \[
    P(R(X_1) = r_1,\,\cdots,\,R(X_n) = r_n) = \frac{1}{n!}
  \]
  Обозначим $R_i = R(X_i)$. 

  Аналогично $Y_1,\,\cdots,\,Y_n$ -- выборка из непрерывного распределения и $S_1,\,\cdots,\,S_n$ -- соответствующие ранги.

  Заметим, что
  \[
    \overline{R} = \overline{S} = \frac{n + 1}{2}
  \]
\end{note}

\begin{definition}
  Величину 
  \[
    \rho_S = \frac{\sum_{i = 1}^n(R_i - \overline{R})(S_i - \overline{S})}{\sqrt{\sum_{i = 1}^n(R_i - \overline{R})^2(S_i - \overline{S})^2}}
  \]
  называют \textbf{коэффициентом корреляции Спирмана}.
\end{definition}

\begin{note}
  Заметим, что
  \[
    \sum_{i = 1}^n(S_i - \overline{S})^2 = \sum_{i = 1}^n(R_i - \overline{R})^2 = \sum_{i = 1}^n\left(R_i - \frac{n + 1}{2}\right)^2 = \sum_{i = 1}^n\left(i - \frac{n + 1}{2}\right)^2 = \frac{n^3 - n}{12}
  \]
\end{note}

Определим $T_i$: переставим пары $(R_i,\,S_i)$ в порядке возрастания первой координаты $\Rightarrow$ получим
\[
  (1,\,T_1),\,\cdots,\,(n,\,T_n)
\]
причём $R_i = k \Leftrightarrow T_k = S_i$.

Тогда
\begin{align*}
  \rho_S = \frac{12}{n^3 - n}\sum_{i = 1}^n(R_i - \frac{n + 1}{2})(S_i - \frac{n + 1}{2}) = \frac{12}{n^3 - n}\sum_{i = 1}^n(i - \frac{n + 1}{2})(T_i - \frac{n + 1}{2}) =\\
  \frac{12}{n^3 - n}(\sum_{i = 1}^n iT_i - \frac{n + 1}{2}\sum_{i = 1}^nT_i - \frac{n + 1}{2}\sum_{i = 1}^ni + n(\frac{n + 1}{2})^2) =\\
  1 - \frac{6}{n^3 - n}\sum_{i = 1}^n(R_i - S_i)^2
\end{align*}
Знаем, что $(T_1,\,\cdots,\,T_n)$ -- перестановка $(1,\,\cdots,\,n)$, и при $H_0$ все $n!$ перестановок равновероятны:
\[
  \forall k :\: \mathbb{E}T_k = \sum_{j = 1}^n jP(T_k = j) = \frac{n + 1}{2}
\]
Тогда очевидно, что
\[
  \mathbb{E}\rho_S = 0
\]

\begin{lemma}
  Свойства $\rho_S$:
  \begin{enumerate}
    \item $H_0 \Rightarrow \begin{cases}
      \mathbb{E}\rho_S = 0\\
      \mathbb{V}\rho_S = \frac{1}{n - 1}
    \end{cases}$
    \item По КБШ: $\rho_S \in [-1,\,1]$, причём крайние значения достигаются (при $R_i = S_i$ и обратном порядке).
    \item При $H_0$ распределение $\rho_S$ известно, не зависит от $F_X$ и $F_Y$ и его квантили есть в таблицах.
    \item $H_0 \Rightarrow \frac{\rho_S}{\sqrt{\mathbb{V}\rho_S}} \overset{d}{\to} \mathcal{N}(0,\,1),\, n \to +\infty$, причём при $n \geq 50$ это приближение используется.
  \end{enumerate}
\end{lemma}

Получается, критерием будет $\{\vert \rho_S\vert > const\}$ -- больше какой-то квантили.

\begin{note}
  Коэффициент корреляции Кендала.

  Опять есть выборки $X_1,\,\cdots,\,X_n$ и $Y_1,\,\cdots,\,Y_n$.
\end{note}

\begin{definition}
  Пары $(X_i,\, Y_i)$ и $(X_j,\, Y_j),\, 1 \leq i < j \leq n$ \textbf{согласованы}, если
  \[
    \text{sgn}(X_i - X_j)\text{sgn}(Y_i - Y_j) = 1
  \]
\end{definition}

Пусть $S$ -- число согласованных пар, $R$ -- число несогласованных. Тогда
\[
  S + R = \frac{n(n - 1)}{2}
\]
Определим
\[
  T := S - R = \sum_{i < j}\text{sgn}(X_i - X_j)(Y_i - Y_j)
\]
Очев $T \in [-\frac{n(n - 1)}{2},\, \frac{n(n - 1)}{2}]$

\begin{definition}
  Величину
  \[
    \tau = \frac{T}{\frac{n(n - 1)}{2}}
  \]
  называют \textbf{коэффициентом корреляции Кендала}
\end{definition}

Заметим, 
\[
  \tau = \frac{2(S - R)}{n(n - 1)} = \frac{2(S + R) - 4R}{n(n - 1)} = 1 - \frac{4R}{n(n - 1)} = 1 - \frac{4}{n(n - 1)}\sum_{i < j}\mathbb{I}\{T_i > T_j\}
\]
\begin{lemma}
  Свойства коэффициента Кэндала:
  \begin{enumerate}
    \item $H_0 \Rightarrow \begin{cases}
      \mathbb{E}T = 0\\
      \mathbb{V}T = \frac{2(2n + 5)}{9n(n - 1)}
    \end{cases}$
    \item $\tau \in [-1,\,1]$, крайние значения достигаются при всех согласованных и всех несогласованных.
    \item \[
      \tau = 1 - \frac{4}{n^2 - n}\sum_{i < j}\mathbb{I}\{T_i > T_j\};\;\;\;\; \rho_S = 1 - \frac{12}{n^3 - n}\sum_{i < j}(j - i)\mathbb{I}\{T_i > T_j\}
    \]
    то есть $\rho_S$ сильнее реагирует на различия рангов в несогласованных парах.
    \item $H_0 \Rightarrow \rho(\tau,\, \rho_S) \overset{n \to +\infty}{\to} 1$
    
    Кроме того, распределение $\tau$ известно, не зависит от $F_X,\, F_Y$ и его квантили известны.
  \end{enumerate}
\end{lemma}

\end{document}